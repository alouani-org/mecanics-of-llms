# Int√©gration Avanc√©e : Agent ReAct avec des LLMs R√©els

Ce document montre comment adapter le script `06_react_agent_bonus.py` pour utiliser des **vrais LLMs** (OpenAI, Anthropic, Ollama, etc.) au lieu de la simulation incluse.

## üìå Table des Mati√®res

1. [OpenAI (GPT-4, GPT-3.5)](#openai)
2. [Anthropic (Claude)](#anthropic)
3. [Groq (inf√©rence ultra-rapide)](#groq)
4. [Ollama (LLMs locaux)](#ollama)
5. [Gestion des Erreurs et Timeouts](#erreurs)
6. [Architecture Robuste](#architecture)

---

## <a name="openai"></a>1Ô∏è‚É£ OpenAI (GPT-4, GPT-3.5)

### Installation

```bash
pip install openai
```

### Configuration

```python
import os
from openai import OpenAI

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class OpenAIAgent(Agent):
    """Agent avec int√©gration OpenAI."""

    def __init__(self, model: str = "gpt-4", **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.client = client

    def _simulate_llm_reasoning(self, task: str, context: str) -> str:
        """Utiliser l'API OpenAI au lieu de la simulation."""
        
        tools_desc = self._format_tools_description()
        
        system_prompt = f"""Tu es un agent autonome capable de raisonner et d'agir.

Tu as acc√®s aux outils suivants:
{tools_desc}

R√©ponds au format suivant:
Thought: [Ton analyse de la situation]
Action: nom_outil(param1=val1, param2=val2) OU Final Answer: [r√©ponse finale]

Sois concis et actionnel."""

        user_message = f"""T√¢che: {task}

Contexte actuel:
{context if context else '[Aucun contexte pr√©c√©dent]'}"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message},
                ],
                temperature=0.7,
                max_tokens=500,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå Erreur OpenAI: {e}")
            return "Thought: Erreur d'appel API.\nFinal Answer: Impossible de traiter la t√¢che."
```

### Utilisation

```python
# Cr√©er un agent OpenAI
agent = OpenAIAgent(name="OpenAI-Agent", model="gpt-4")

# Enregistrer les outils (comme avant)
agent.register_tool(...)

# Ex√©cuter
result = agent.run("Calcule 5 + 3")
```

### Co√ªts Estim√©s

- **GPT-4** : ~$0.03 / 1K input tokens, ~$0.06 / 1K output tokens
- **GPT-3.5-turbo** : ~$0.0005 / 1K input tokens, ~$0.0015 / 1K output tokens

---

## <a name="anthropic"></a>2Ô∏è‚É£ Anthropic (Claude)

### Installation

```bash
pip install anthropic
```

### Configuration

```python
import os
from anthropic import Anthropic

client = Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

class ClaudeAgent(Agent):
    """Agent avec int√©gration Anthropic Claude."""

    def __init__(self, model: str = "claude-3-opus-20240229", **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.client = client

    def _simulate_llm_reasoning(self, task: str, context: str) -> str:
        """Utiliser l'API Anthropic au lieu de la simulation."""
        
        tools_desc = self._format_tools_description()
        
        system_prompt = f"""Tu es un agent autonome expert en raisonnement et en planification.

Tu as acc√®s aux outils suivants:
{tools_desc}

Format de r√©ponse:
Thought: [Ton analyse]
Action: nom_outil(param1=val1) OU Final Answer: [r√©ponse]"""

        messages = [
            {
                "role": "user",
                "content": f"""T√¢che: {task}

Contexte:
{context if context else '[Nouveau contexte]'}""",
            }
        ]

        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=1024,
                system=system_prompt,
                messages=messages,
            )
            return response.content[0].text
        except Exception as e:
            print(f"‚ùå Erreur Anthropic: {e}")
            return "Thought: Erreur API.\nFinal Answer: Service indisponible."
```

### Utilisation

```python
# Claude 3 Opus (le plus puissant)
agent = ClaudeAgent(model="claude-3-opus-20240229")

# Ou Claude 3 Sonnet (plus rapide, moins cher)
agent = ClaudeAgent(model="claude-3-sonnet-20240229")
```

### Avantages de Claude

- ‚úÖ **Contexte long** : jusqu'√† 200K tokens
- ‚úÖ **Raisonnement sup√©rieur** : particuli√®rement bon pour les agents complexes
- ‚úÖ **Moins de hallucinations** : g√©n√©ralement plus fiable
- ‚úÖ **Vision** : Claude 3 supporte les images

---

## <a name="groq"></a>3Ô∏è‚É£ Groq (Inf√©rence Ultra-Rapide)

### Installation

```bash
pip install groq
```

### Configuration

```python
import os
from groq import Groq

client = Groq(api_key=os.getenv("GROQ_API_KEY"))

class GroqAgent(Agent):
    """Agent avec inf√©rence Groq (tr√®s rapide)."""

    def __init__(self, model: str = "mixtral-8x7b-32768", **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.client = client

    def _simulate_llm_reasoning(self, task: str, context: str) -> str:
        """Groq : inf√©rence extr√™mement rapide."""
        
        tools_desc = self._format_tools_description()

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": f"Agent autonome. Outils:\n{tools_desc}",
                    },
                    {"role": "user", "content": f"T√¢che: {task}\n\nContexte: {context}"},
                ],
                temperature=0.7,
                max_tokens=512,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå Erreur Groq: {e}")
            return "Thought: Erreur.\nFinal Answer: R√©essayez."
```

### Utilisation & Avantages

```python
agent = GroqAgent(model="mixtral-8x7b-32768")  # ~500ms latence!
```

**Avantages:**
- ‚ö° **Ultra-rapide** : 100-200 tokens/sec
- üí∞ **Gratuit** : jusqu'√† certains quotas
- üìä **Bon pour les agents** : latence basse = meilleure r√©activit√©

---

## <a name="ollama"></a>4Ô∏è‚É£ Ollama (LLMs Locaux)

### Installation

```bash
# T√©l√©charger Ollama depuis https://ollama.ai
ollama pull mistral        # ou llama2, neural-chat, etc.
ollama serve               # D√©marre le serveur sur localhost:11434
```

### Configuration

```python
import requests

class OllamaAgent(Agent):
    """Agent avec Ollama (LLM local)."""

    def __init__(self, model: str = "mistral", host: str = "localhost:11434", **kwargs):
        super().__init__(**kwargs)
        self.model = model
        self.host = host

    def _simulate_llm_reasoning(self, task: str, context: str) -> str:
        """Utiliser Ollama en local."""
        
        tools_desc = self._format_tools_description()
        prompt = f"""Tu es un agent autonome.

Outils:
{tools_desc}

T√¢che: {task}
Contexte: {context}

R√©ponds au format:
Thought: ...
Action: ..."""

        try:
            response = requests.post(
                f"http://{self.host}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                },
                timeout=30,
            )
            return response.json()["response"]
        except Exception as e:
            print(f"‚ùå Erreur Ollama: {e}")
            return "Thought: Erreur locale.\nFinal Answer: Serveur indisponible."
```

### Utilisation & Avantages

```python
agent = OllamaAgent(model="mistral")  # Mistral 7B en local
```

**Avantages:**
- üîí **Priv√©** : tout s'ex√©cute en local
- üí∞ **Gratuit** : une fois t√©l√©charg√©
- üöÄ **Rapide** : GPU acc√©l√©ration possible
- ‚ö†Ô∏è **Limitation** : moins performant que GPT-4

---

## <a name="erreurs"></a>5Ô∏è‚É£ Gestion Robuste des Erreurs et Timeouts

### Exemple Avanc√©

```python
import time
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustAgent(Agent):
    """Agent avec gestion d'erreurs robuste."""

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
    )
    def _call_llm_with_retry(self, task: str, context: str) -> str:
        """Appeler l'LLM avec retry automatique."""
        return self._simulate_llm_reasoning(task, context)

    def run(self, task: str, verbose: bool = True) -> str:
        """Ex√©cution avec gestion d'erreurs."""
        try:
            context = ""
            for iteration in range(1, self.max_iterations + 1):
                try:
                    # Appel avec retry
                    response = self._call_llm_with_retry(task, context)
                    
                    action, thought = self._parse_action(response)
                    if not action:
                        print("‚ö†Ô∏è Pas d'action g√©n√©r√©e")
                        break

                    if action.startswith("Final Answer:"):
                        return action.replace("Final Answer:", "").strip()

                    observation = self._execute_action(action)
                    context += f"\nIt√©ration {iteration}: {observation}"

                except TimeoutError:
                    print(f"‚ùå Timeout √† l'it√©ration {iteration}, r√©essai...")
                    continue
                except Exception as e:
                    print(f"‚ùå Erreur √† l'it√©ration {iteration}: {e}")
                    if iteration == self.max_iterations:
                        raise
                    continue

            return "It√©rations maximales atteintes sans r√©ponse."

        except Exception as e:
            print(f"‚ùå Erreur fatale: {e}")
            return f"Erreur: {e}"
```

### Installation pour les Retries

```bash
pip install tenacity
```

---

## <a name="architecture"></a>6Ô∏è‚É£ Architecture Robuste pour la Production

### Structure Recommand√©e

```python
from dataclasses import dataclass
from typing import Optional
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class AgentConfig:
    """Configuration centralis√©e de l'agent."""
    name: str
    model: str
    provider: str  # "openai", "anthropic", "groq", "ollama"
    max_iterations: int = 10
    timeout: int = 30
    temperature: float = 0.7
    api_key: Optional[str] = None


class ProductionAgent(Agent):
    """Agent optimis√© pour la production."""

    def __init__(self, config: AgentConfig):
        super().__init__(name=config.name, max_iterations=config.max_iterations)
        self.config = config
        self._init_llm_client()

    def _init_llm_client(self):
        """Initialiser le client LLM bas√© sur le provider."""
        if self.config.provider == "openai":
            from openai import OpenAI
            self.client = OpenAI(api_key=self.config.api_key)
        elif self.config.provider == "anthropic":
            from anthropic import Anthropic
            self.client = Anthropic(api_key=self.config.api_key)
        # ... autres providers

    def _call_llm(self, prompt: str) -> str:
        """Appel unifi√© avec logging."""
        logger.info(f"Appel LLM ({self.config.provider}), tokens: ~{len(prompt)//4}")
        # Impl√©mentation
        pass

    def run(self, task: str, verbose: bool = True) -> str:
        """Ex√©cution avec logging structur√©."""
        logger.info(f"D√©marrage agent: {self.config.name}, t√¢che: {task}")
        result = super().run(task, verbose)
        logger.info(f"Fin agent: {result[:100]}...")
        return result


# Utilisation
config = AgentConfig(
    name="Production-Agent",
    model="gpt-4",
    provider="openai",
    api_key="sk-...",
)
agent = ProductionAgent(config)
```

---

## üìä Comparaison des Providers

| Provider | Latence | Co√ªt | Qualit√© | Contexte | Local |
|----------|---------|------|---------|----------|-------|
| **OpenAI GPT-4** | 2-5s | $$ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 128K | ‚ùå |
| **Claude 3 Opus** | 2-4s | $$ | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 200K | ‚ùå |
| **Groq Mixtral** | 0.5s | Free | ‚≠ê‚≠ê‚≠ê‚≠ê | 32K | ‚ùå |
| **Mistral 7B (Ollama)** | 1-3s | Free | ‚≠ê‚≠ê‚≠ê | 4K | ‚úÖ |
| **GPT-3.5-turbo** | 1-2s | $ | ‚≠ê‚≠ê‚≠ê‚≠ê | 128K | ‚ùå |

---

## üéØ Recommandations

- **D√©veloppement / Testing** ‚Üí Groq (rapide, gratuit)
- **Agents complexes** ‚Üí Claude (meilleur raisonnement)
- **Production scalable** ‚Üí OpenAI (fiable, API robuste)
- **Privacy critique** ‚Üí Ollama (local)
- **Co√ªt sensible** ‚Üí GPT-3.5-turbo ou Ollama

---

**Bon d√©veloppement !** üöÄ
