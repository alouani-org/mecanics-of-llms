#!/usr/bin/env python
"""
Script 3 : Softmax et Temp√©rature (Chapitres 7 & 11).

Ce script montre comment la temp√©rature modifie la distribution softmax :
- √Ä basse temp√©rature (T < 1), la distribution devient pointue ‚Üí greedy.
- √Ä haute temp√©rature (T > 1), la distribution s'aplatit ‚Üí diversit√©.
- L'effet sur l'entropie (mesure de dispersion).

D√©pendances :
    pip install torch numpy
    pip install matplotlib  # optionnel, pour les graphiques

Utilisation :
    python 03_temperature_softmax.py
"""

import torch
import torch.nn.functional as F
import numpy as np


def plot_temperature(logits, temperatures):
    """Visualise l'effet de la temp√©rature (n√©cessite matplotlib)."""
    try:
        import matplotlib.pyplot as plt

        probabilities_list = []
        entropies = []

        for T in temperatures:
            probs = F.softmax(logits / T, dim=0).numpy()
            probabilities_list.append(probs)

            # Entropie Shannon
            entropy = -np.sum(probs * np.log(probs + 1e-10))
            entropies.append(entropy)

        # Plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))

        # Subplot 1 : Probabilit√©s par temp√©rature
        x = np.arange(len(logits))
        width = 0.15

        for i, T in enumerate(temperatures):
            ax1.bar(x + i * width, probabilities_list[i], width, label=f"T={T}")

        ax1.set_xlabel("Token ID")
        ax1.set_ylabel("Probabilit√©")
        ax1.set_title("Effet de la temp√©rature sur softmax")
        ax1.legend()
        ax1.grid(axis="y", alpha=0.3)

        # Subplot 2 : Entropie vs Temp√©rature
        ax2.plot(temperatures, entropies, "o-", linewidth=2, markersize=8)
        ax2.set_xlabel("Temp√©rature")
        ax2.set_ylabel("Entropie Shannon")
        ax2.set_title("Entropie augmente avec la temp√©rature")
        ax2.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig("temperature_effect.png", dpi=100)
        print("\n‚úÖ Graphique sauvegard√©: temperature_effect.png\n")
        return True
    except ImportError:
        print("\n‚ö†Ô∏è  matplotlib non install√©. Graphique ignor√©.")
        print("   Pour voir le graphique: pip install matplotlib\n")
        return False


def main():
    # Logits simplifi√©s (comme la sortie du mod√®le avant softmax)
    # Imagine que ce sont les scores pour [chat, chien, souris, oiseau]
    logits = torch.tensor([2.0, 1.0, 0.5, 0.1])
    token_names = ["chat", "chien", "souris", "oiseau"]
    temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]

    print("=" * 70)
    print("EFFET DE LA TEMP√âRATURE SUR LA DISTRIBUTION SOFTMAX")
    print("=" * 70 + "\n")

    print(f"Logits bruts: {logits.numpy()}")
    print(f"Tokens: {token_names}\n")

    probabilities_list = []
    entropies = []

    print("=" * 70)
    print("R√âSULTATS PAR TEMP√âRATURE")
    print("=" * 70 + "\n")

    for T in temperatures:
        probs = F.softmax(logits / T, dim=0)
        probabilities_list.append(probs.numpy())

        # Entropie Shannon : -sum(p * log(p))
        entropy = -np.sum(probs.numpy() * np.log(probs.numpy() + 1e-10))
        entropies.append(entropy)

        print(f"Temp√©rature = {T}")
        print(f"  Probabilit√©s (normalis√©es):")
        for name, prob in zip(token_names, probs.numpy()):
            bar = "‚ñà" * int(prob * 50)  # Barre simple
            print(f"    {name:8s}: {prob:.3f}  {bar}")
        print(f"  Entropie: {entropy:.3f}")
        print()

    # Visualisation optionnelle
    print("=" * 70)
    print("VISUALISATION")
    print("=" * 70)

    has_plot = plot_temperature(logits, temperatures)

    # Interpr√©tation
    print("=" * 70)
    print("INTERPR√âTATION")
    print("=" * 70 + "\n")

    print("‚úì √Ä T=0.1 (basse temp√©rature):")
    print("  ‚Üí 'chat' domine largement (greedy decoding).")
    print("  ‚Üí Faible entropie ‚Üí Sortie d√©terministe et r√©p√©titive.\n")

    print("‚úì √Ä T=1.0 (temp√©rature neutre):")
    print("  ‚Üí Distribution 'naturelle' selon les logits.")
    print("  ‚Üí C'est la temp√©rature par d√©faut.\n")

    print("‚úì √Ä T=5.0 (haute temp√©rature):")
    print("  ‚Üí Distribution quasi-uniforme (tous les tokens presque √©gaux).")
    print("  ‚Üí Haute entropie ‚Üí Diversit√©, mais aussi incoh√©rence.\n")

    print("=" * 70)
    print("APPLICATIONS PRATIQUES")
    print("=" * 70 + "\n")

    print("üìå R√©capitulatif rapidement:")
    print(f"  Entropie min (T={temperatures[0]}): {entropies[0]:.3f}")
    print(f"  Entropie max (T={temperatures[-1]}): {entropies[-1]:.3f}")
    print()
    print("  ‚Üí Augmenter T de 0.1 √† 5.0 multiplie l'entropie par")
    print(f"    {entropies[-1] / (entropies[0] + 1e-10):.1f}x")
    print()
    print("  Pi√®ges courants:")
    print("    üî¥ T=0 sur GPU n'est pas parfaitement d√©terministe (arrondis float).")
    print("    üî¥ T trop √©lev√© ‚Üí hallucinations et incoh√©rence.")
    print("    üü¢ T=0.7-0.9 : bon compromis cr√©ativit√©/stabilit√©.")


if __name__ == "__main__":
    main()
