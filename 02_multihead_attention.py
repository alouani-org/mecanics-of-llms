#!/usr/bin/env python
"""
Script 2 : Self-Attention et Multi-Head (Chapitre 3).

Ce script simule une couche d'attention multi-tÃªte minimale, permettant de :
- Comprendre les projections Q, K, V.
- Voir comment chaque tÃªte focalise sur diffÃ©rentes dÃ©pendances.
- VÃ©rifier que les poids d'attention somment Ã  1 (distribution de probabilitÃ©).

DÃ©pendances :
    pip install torch numpy

Utilisation :
    python 02_multihead_attention.py
"""

import torch
import torch.nn.functional as F
import numpy as np


def main():
    # ParamÃ¨tres
    batch_size = 1
    seq_len = 4        # Longueur de sÃ©quence ("Le chat dort bien")
    d_model = 64       # Dimension du modÃ¨le
    num_heads = 2      # Nombre de tÃªtes d'attention
    d_head = d_model // num_heads

    print("=" * 60)
    print("MULTI-HEAD ATTENTION (SIMULATION SIMPLIFIÃ‰E)")
    print("=" * 60 + "\n")

    # Exemple concret avec noms de tokens
    token_names = ["Le", "chat", "dort", "bien"]
    print(f"Phrase d'exemple: {' '.join(token_names)}\n")

    # CrÃ©er une sÃ©quence d'embeddings (simulÃ©e)
    # En pratique, ce sont les sorties des couches prÃ©cÃ©dentes
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"EntrÃ©e x shape: {x.shape}")
    print(f"  (batch={batch_size}, seq_len={seq_len}, d_model={d_model})\n")

    # Projections linÃ©aires pour Q, K, V
    W_q = torch.randn(d_model, d_model)
    W_k = torch.randn(d_model, d_model)
    W_v = torch.randn(d_model, d_model)

    Q = x @ W_q  # [batch, seq_len, d_model]
    K = x @ W_k
    V = x @ W_v

    print(f"Q, K, V shapes: {Q.shape}\n")

    # Calcul de l'attention par tÃªte
    print("=" * 60)
    print("CALCUL DE L'ATTENTION PAR TÃŠTE")
    print("=" * 60 + "\n")

    attention_outputs = []

    for head_idx in range(num_heads):
        start, end = head_idx * d_head, (head_idx + 1) * d_head
        Q_h = Q[:, :, start:end]  # [batch, seq_len, d_head]
        K_h = K[:, :, start:end]
        V_h = V[:, :, start:end]

        # Attention(Q, K, V) = softmax(Q @ K^T / sqrt(d_head)) @ V
        scores = Q_h @ K_h.transpose(-2, -1) / np.sqrt(d_head)  # [batch, seq_len, seq_len]
        attention_weights = F.softmax(scores, dim=-1)            # [batch, seq_len, seq_len]
        output_h = attention_weights @ V_h                       # [batch, seq_len, d_head]

        attention_outputs.append(output_h)

        print(f"TÃªte {head_idx}:")
        print(f"  Scores (bruts): shape {scores.shape}")
        print(f"  Poids d'attention (aprÃ¨s softmax):")
        print(f"    {attention_weights[0].detach().numpy()}")
        print(f"  Somme des poids pour chaque token:")
        print(f"    {attention_weights[0].sum(dim=1).detach().numpy()}")
        print(f"  (VÃ©rification: chaque ligne doit sommer Ã  ~1.0)")
        print()

    # ConcatÃ©ner toutes les tÃªtes
    output = torch.cat(attention_outputs, dim=-1)  # [batch, seq_len, d_model]

    print("=" * 60)
    print("RÃ‰SULTAT FINAL")
    print("=" * 60 + "\n")
    print(f"Sortie concatÃ©nÃ©e: {output.shape}")
    print(f"(Les {num_heads} tÃªtes sont rÃ©unies pour un vecteur final par token)")

    print("\nðŸ’¡ INTUITION:")
    print("  â€¢ Chaque tÃªte capture DIFFÃ‰RENTES dÃ©pendances dans la phrase.")
    print(f"  â€¢ Avec nos tokens {token_names}:")
    print("    - TÃªte 0 peut se concentrer sur 'Le â†’ chat' (sujet-verbe).")
    print("    - TÃªte 1 peut se concentrer sur 'chat â†’ dort' (verbe-adverbe).")
    print("  â€¢ La fusion permet au modÃ¨le de combiner ces perspectives.")
    print(f"\n  Observation: Chaque tÃªte assigne des poids d'attention diffÃ©rents.")
    print(f"  Exemple avec {token_names[1]} (token 1):")
    print(f"    - TÃªte 0: 'chat' regarde surtout vers 'Le' et 'dort'")
    print(f"    - TÃªte 1: 'chat' regarde plus vers 'bien'")
    print(f"  â†’ Perspectives complÃ©mentaires = reprÃ©sentation riche!")


if __name__ == "__main__":
    main()
