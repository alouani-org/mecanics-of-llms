# LlamaIndex Guide Complet - RAG AvancÃ©

Ce guide explique comment utiliser le script `07_llamaindex_rag_advanced.py` et intÃ©grer LlamaIndex dans vos projets.

## ğŸ“‹ Table des MatiÃ¨res

1. [Qu'est-ce que LlamaIndex?](#intro)
2. [Installation](#installation)
3. [Concepts ClÃ©s](#concepts)
4. [ExÃ©cution du Script](#execution)
5. [Cas d'Usage AvancÃ©s](#usecases)
6. [IntÃ©gration avec OpenAI](#openai)
7. [Troubleshooting](#troubleshooting)

---

## <a name="intro"></a>1ï¸âƒ£ Qu'est-ce que LlamaIndex?

**LlamaIndex** (anciennement GPT Index) est un framework pour construire des applications LLM avancÃ©es.

**Cas d'usage:**
- ğŸ”„ **RAG (Retrieval-Augmented Generation)**: Augmenter LLMs avec donnÃ©es externes
- ğŸ“„ **Document Q&A**: Poser des questions sur des documents
- ğŸ¤– **Chatbots intelligents**: Agents avec mÃ©moire et contexte
- ğŸ“Š **Data analysis**: Extraire insights de donnÃ©es non-structurÃ©es
- ğŸ”— **Knowledge graphs**: Construire des graphes de connaissances

**Architectures supportÃ©es:**
```
Data Sources (PDF, Web, Database)
         â†“
    LlamaIndex
         â†“
    (Parsing + Embedding)
         â†“
Vector Store (Pinecone, Weaviate, Chroma, etc.)
         â†“
Query Engine / Retriever
         â†“
LLM (OpenAI, Claude, Ollama, etc.)
         â†“
RÃ©ponse final
```

---

## <a name="installation"></a>2ï¸âƒ£ Installation

### Installation Minimale (DÃ©mo)

```bash
# Script dÃ©mo sans dÃ©pendances externes
python examples/07_llamaindex_rag_advanced.py
```

La dÃ©mo fonctionne **sans aucune installation supplÃ©mentaire** (embeddings simulÃ©s).

### Installation ComplÃ¨te (Production)

```bash
# Installation de base
pip install llama-index

# Avec OpenAI
pip install llama-index openai

# Avec support de documents (PDF, Word, etc.)
pip install llama-index-readers-file python-pptx openpyxl

# Avec vector stores
pip install llama-index-vector-stores-pinecone
pip install llama-index-vector-stores-weaviate

# Avec autres LLMs
pip install llama-index-llms-anthropic
pip install llama-index-llms-groq
pip install llama-index-llms-ollama
```

### Configuration OpenAI

```bash
# Linux/Mac
export OPENAI_API_KEY="sk-..."

# PowerShell Windows
$env:OPENAI_API_KEY="sk-..."

# Ou dans un fichier .env
OPENAI_API_KEY=sk-...
```

---

## <a name="concepts"></a>3ï¸âƒ£ Concepts ClÃ©s

### A) Document Loading

```python
from llama_index.core import Document, SimpleDirectoryReader

# Charger depuis fichiers
documents = SimpleDirectoryReader("./data").load_data()

# Ou crÃ©er manuellement
doc = Document(
    text="Contenu du document",
    metadata={"source": "manual", "date": "2025-01"}
)
```

### B) Vector Index

```python
from llama_index.core import VectorStoreIndex

# CrÃ©er un index Ã  partir de documents
index = VectorStoreIndex.from_documents(
    documents,
    embed_model=embed_model  # Choisir modÃ¨le d'embedding
)

# Sauvegarder pour rÃ©utilisation
index.storage_context.persist("./storage")

# Charger
from llama_index.core import StorageContext, load_index_from_storage
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### C) Query Engine

```python
# CrÃ©er un query engine
query_engine = index.as_query_engine(
    similarity_top_k=3  # Retriever les 3 documents les plus pertinents
)

# ExÃ©cuter une requÃªte
response = query_engine.query("Qu'est-ce qu'un Transformer?")
print(response)
```

### D) Chat Engine (avec mÃ©moire)

```python
# Chat avec historique conversationnel
chat_engine = index.as_chat_engine(
    chat_mode="condense_question",  # Reformuler Q avec contexte
    memory=ChatMemoryBuffer(token_limit=3900)
)

# Messages
response = chat_engine.chat("Parle-moi des Transformers")
response = chat_engine.chat("Et les mÃ©canismes d'attention?")  # Avec contexte!
```

### E) Hybrid Search (BM25 + Vector)

```python
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.retrievers.bm25 import BM25Retriever

# Combiner keyword (BM25) + semantic (vector) search
bm25_retriever = BM25Retriever.from_defaults(
    docstore=docstore, 
    nodes=nodes
)
vector_retriever = index.as_retriever(similarity_top_k=3)

fusion_retriever = QueryFusionRetriever(
    [bm25_retriever, vector_retriever],
    similarity_top_k=3,
    query_gen_prompt="Reformule cette question de 3 faÃ§ons..."
)

# Retriever les meilleurs results
nodes = fusion_retriever.retrieve("Qu'est-ce qu'un Transformer?")
```

---

## <a name="execution"></a>4ï¸âƒ£ ExÃ©cution du Script

### ExÃ©cution Basique

```bash
cd examples
python 07_llamaindex_rag_advanced.py
```

**Output attendu:**

```
================================================================================
ğŸ¦™ LlamaIndex RAG Advanced Demo
================================================================================

ğŸ“š Phase 1: Chargement des documents
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Transformers : Architecture (1234 chars)
  âœ“ Attention Multi-TÃªte (890 chars)
  âœ“ Fine-tuning et Adaptation (1050 chars)

ğŸ” Phase 2: CrÃ©ation de l'index vectoriel
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ Index crÃ©Ã© avec 3 documents
  âœ“ Dimension embedding: 384

âš™ï¸  Phase 3: Initialisation du RAG Engine
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  âœ“ RAG Engine prÃªt

ğŸ’¬ Phase 4: RequÃªtes RAG
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Q1: Qu'est-ce qu'un Transformer?
ğŸ“„ Documents retrievÃ©s:
   - Transformers : Architecture (a1b2c3d4)
   - Attention Multi-TÃªte (e5f6g7h8)

ğŸ¤– RÃ©ponse:
D'aprÃ¨s le contexte fourni, les Transformers sont des architectures basÃ©es
sur l'attention qui traitent tous les tokens en parallÃ¨le, contrairement aux RNNs...

[Suite de la dÃ©monstration...]

âœ… DÃ©mo complÃ©tÃ©e!
ğŸ’¾ RÃ©sultats exportÃ©s dans: rag_results.json
```

### RÃ©sultat JSON

Le script gÃ©nÃ¨re `rag_results.json`:

```json
{
  "timestamp": "2025-01-10T14:23:45.123456",
  "queries": [
    {
      "question": "Qu'est-ce qu'un Transformer?",
      "retrieved_docs": [
        {
          "title": "Transformers : Architecture",
          "id": "a1b2c3d4",
          "snippet": "Le Transformer est une architecture de rÃ©seau profond..."
        }
      ],
      "answer": "D'aprÃ¨s le contexte...",
      "timestamp": "2025-01-10T14:23:45.123456"
    }
  ],
  "statistics": {
    "total_queries": 3,
    "total_turns": 3,
    "documents_indexed": 3
  }
}
```

---

## <a name="usecases"></a>5ï¸âƒ£ Cas d'Usage AvancÃ©s

### A) RAG sur PDFs

```python
from llama_index.core import SimpleDirectoryReader

# Charger tous les PDFs
documents = SimpleDirectoryReader("./pdfs/").load_data()

# CrÃ©er l'index
index = VectorStoreIndex.from_documents(documents)

# RequÃªte
query_engine = index.as_query_engine()
response = query_engine.query("Quel est le chapitre 3?")
```

### B) Chat Multi-Tour avec MÃ©moire

```python
from llama_index.core.memory import ChatMemoryBuffer

chat_engine = index.as_chat_engine(
    chat_mode="condense_question",
    memory=ChatMemoryBuffer(token_limit=3900),
    system_prompt="Tu es un expert en IA..."
)

# Tour 1
response1 = chat_engine.chat("Parle des Transformers")
# Chat conserve le contexte

# Tour 2
response2 = chat_engine.chat("Qu'en est-il des MoE?")
# La question est augmentÃ©e avec le contexte du tour 1
```

### C) Evaluation RAG

```python
from llama_index.core.evaluation import (
    RelevancyEvaluator,
    FaithfulnessEvaluator
)

evaluator_relevancy = RelevancyEvaluator()
evaluator_faithfulness = FaithfulnessEvaluator()

# Ã‰valuer une rÃ©ponse
eval_result = evaluator_relevancy.evaluate_response(
    query="Qu'est-ce qu'un Transformer?",
    response=response
)

print(f"Pertinence: {eval_result.score}")
print(f"Raison: {eval_result.feedback}")
```

### D) Agents avec Outils

```python
from llama_index.core.agent import ReActAgent
from llama_index.core.tools import FunctionTool

# DÃ©finir des outils
def calculator(a: int, b: int, op: str) -> int:
    if op == "+": return a + b
    if op == "*": return a * b
    return 0

tools = [
    FunctionTool.from_defaults(fn=calculator),
    # ... autres outils
]

# CrÃ©er un agent ReAct
agent = ReActAgent.from_tools(tools, llm=llm)

# ExÃ©cuter
response = agent.chat("Calcule 5 + 3 puis multiplie par 2")
```

### E) Hybrid Search pour Meilleures RÃ©sultats

```python
from llama_index.core.retrievers import QueryFusionRetriever
from llama_index.retrievers.bm25 import BM25Retriever

# CrÃ©er les retrievers
bm25_retriever = BM25Retriever.from_defaults(nodes=nodes)
vector_retriever = index.as_retriever(similarity_top_k=3)

# Fusionner
fusion_retriever = QueryFusionRetriever(
    [bm25_retriever, vector_retriever],
    similarity_top_k=3
)

# Utiliser
nodes = fusion_retriever.retrieve("Transformer attention")
```

---

## <a name="openai"></a>6ï¸âƒ£ IntÃ©gration avec OpenAI

### Configuration RecommandÃ©e

```python
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# LLM
Settings.llm = OpenAI(
    model="gpt-4",
    temperature=0.7,
    max_tokens=2048
)

# Embeddings
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small",  # Moins cher, bon pour RAG
)

# Ou GPT-4o pour meilleure qualitÃ©
# Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-large")
```

### CoÃ»ts EstimÃ©s

| ModÃ¨le | Input (1K tokens) | Output (1K tokens) |
|--------|-------------------|-------------------|
| GPT-4o mini | $0.00015 | $0.0006 |
| text-embedding-3-small | $0.00002 | - |
| text-embedding-3-large | $0.00013 | - |

**Exemple pour 1000 requÃªtes:**
- LLM (gpt-4o-mini): ~$0.75
- Embeddings (small): ~$0.02
- **Total: ~$0.77 pour 1000 requÃªtes** âœ…

---

## <a name="troubleshooting"></a>7ï¸âƒ£ Troubleshooting

### âŒ "ModuleNotFoundError: No module named 'llama_index'"

**Solution:**
```bash
pip install llama-index
```

### âŒ "OpenAI API key not found"

**Solution:**
```bash
# VÃ©rifier que la clÃ© est dÃ©finie
echo $OPENAI_API_KEY  # Linux/Mac
echo $env:OPENAI_API_KEY  # PowerShell

# Ou crÃ©er un .env
echo 'OPENAI_API_KEY=sk-...' > .env
```

### âŒ "Embedding model is required"

**Solution:**
```python
from llama_index.core import Settings
from llama_index.embeddings.openai import OpenAIEmbedding

Settings.embed_model = OpenAIEmbedding()
```

### âŒ "No documents found in directory"

**Solution:**
```bash
# VÃ©rifier le rÃ©pertoire
ls -la ./data/

# Ou spÃ©cifier le chemin absolu
from llama_index.core import SimpleDirectoryReader
docs = SimpleDirectoryReader("/absolute/path/to/data").load_data()
```

### âš ï¸ Embeddings lents

**Solution:**
```python
# Utiliser un modÃ¨le plus rapide
Settings.embed_model = OpenAIEmbedding(
    model="text-embedding-3-small"  # Plus rapide que large
)

# Ou Ollama en local
from llama_index.embeddings.ollama import OllamaEmbedding
Settings.embed_model = OllamaEmbedding(model_name="nomic-embed-text")
```

### ğŸ¢ Query Engine lent

**Solutions:**
1. RÃ©duire `similarity_top_k`:
```python
query_engine = index.as_query_engine(similarity_top_k=2)  # au lieu de 5
```

2. Utiliser cache:
```python
from llama_index.core.cache import GPTCache
from gptcache import Cache

gptcache = GPTCache()
Settings.cache = gptcache
```

3. Batch plusieurs requÃªtes:
```python
responses = []
for query in queries:
    response = query_engine.query(query)
    responses.append(response)
```

---

## ğŸ“š Ressources AvancÃ©es

- **Docs officielles**: https://docs.llamaindex.ai/
- **Community**: https://discord.gg/dGcwcsnxhU
- **GitHub**: https://github.com/run-llama/llama_index
- **Exemples**: https://github.com/run-llama/llama_index/tree/main/examples

---

## ğŸ¯ Prochaines Ã‰tapes

1. âœ… ExÃ©cuter le script dÃ©mo: `python examples/07_llamaindex_rag_advanced.py`
2. âœ… Installer LlamaIndex: `pip install llama-index openai`
3. âœ… Charger vos propres documents: `SimpleDirectoryReader("./data")`
4. âœ… Configurer OpenAI: `export OPENAI_API_KEY=sk-...`
5. âœ… IntÃ©grer dans votre app: Voir Cas d'Usage AvancÃ©s
6. âœ… Ã‰valuer la qualitÃ©: Utiliser `RelevancyEvaluator`, `FaithfulnessEvaluator`

---

**Bon dÃ©veloppement!** ğŸš€
