# Scripts Pratiques : ExpÃ©rimenter les Concepts LLM

ğŸŒ [English](../en/README.md) | ğŸ“– **FranÃ§ais** | ğŸ‡ªğŸ‡¸ [EspaÃ±ol](../es/README.md) | ğŸ‡§ğŸ‡· [PortuguÃªs](../pt/README.md)

Collection de **10 scripts Python exÃ©cutables** pour expÃ©rimenter les concepts clÃ©s du livre **"La MÃ©canique des LLM"**.

> ğŸ“š **Ã€ propos** : Ces scripts accompagnent les chapitres du livre. Voir [Parcours PÃ©dagogique](PEDAGOGICAL_JOURNEY.md) pour les correspondances dÃ©taillÃ©es.

**ğŸ“• Acheter le livre :**
- **BrochÃ©** : [Amazon](https://amzn.eu/d/3oREERI)
- **Kindle** : [Amazon](https://amzn.eu/d/b7sG5iw)

---

## ğŸ“‹ Vue d'Ensemble des Scripts

| # | Script | Chapitre(s) | Concepts | Status |
|---|--------|-----------|----------|--------|
| 1 | `01_tokenization_embeddings.py` | 2 | Tokenisation, impact sur la longueur de sÃ©quence | âœ… |
| 2 | `02_multihead_attention.py` | 3 | Self-attention, multi-head, poids d'attention | âœ… |
| 3 | `03_temperature_softmax.py` | 7, 11 | TempÃ©rature, softmax, entropie | âœ… |
| 4 | `04_rag_minimal.py` | 13 | Pipeline RAG, retrieval, similaritÃ© cosinus | âœ… |
| 5 | `05_pass_at_k_evaluation.py` | 12 | Pass@k, Pass^k, Ã©valuation de modÃ¨les | âœ… |
| ğŸ 6 | `06_react_agent_bonus.py` | 14, 15 | **Agents ReAct, tool registration, MCP** | âœ… BONUS |
| ğŸ 7 | `07_llamaindex_rag_advanced.py` | 13, 14 | **RAG avancÃ©, indexing, chat persistant** | âœ… BONUS |
| ğŸ 8 | `08_lora_finetuning_example.py` | 9, 10 | **LoRA, QLoRA, fine-tuning comparatif** | âœ… BONUS |
| ğŸ† **9** | `09_mini_assistant_complet.py` | **11-15** | **ğŸ¯ Projet Final IntÃ©grateur** | âœ… FLAGSHIP |
| ğŸ 10 | `10_activation_steering_demo.py` | 10 | **Activation Steering, 3SO, vecteurs de concept** | âœ… BONUS |

---

## ï¿½ Descriptions DÃ©taillÃ©es des Scripts

### ğŸ“Œ Script 01 : Tokenisation et Embeddings
**Fichier :** `01_tokenization_embeddings.py` | **Chapitre :** 2

**Ce que fait le script :**
- Charge un tokenizer (GPT-2 ou LLaMA-2) et analyse diffÃ©rents textes
- Compare le nombre de tokens entre franÃ§ais et anglais
- DÃ©montre l'impact de la longueur de sÃ©quence sur le coÃ»t computationnel

**Ce que vous apprenez :**
- Comment le texte est dÃ©coupÃ© en tokens (BPE, WordPiece)
- Pourquoi "Bonjour" peut devenir 2-3 tokens alors que "Hello" n'en fait qu'un
- L'impact direct : plus de tokens = coÃ»t O(nÂ²) plus Ã©levÃ© pour l'attention

**Sortie attendue :**
```
Text: L'IA est utile
  Token count: 5
  Tokens: ['L', "'", 'IA', 'est', 'utile']
```

---

### ğŸ“Œ Script 02 : Attention Multi-TÃªtes
**Fichier :** `02_multihead_attention.py` | **Chapitre :** 3

**Ce que fait le script :**
- Simule une couche d'attention multi-tÃªtes avec des tenseurs PyTorch
- Calcule les projections Q, K, V et les poids d'attention
- Affiche comment chaque tÃªte "regarde" diffÃ©remment la phrase

**Ce que vous apprenez :**
- Le mÃ©canisme Q (Query), K (Key), V (Value)
- Pourquoi plusieurs tÃªtes capturent des dÃ©pendances diffÃ©rentes
- Que les poids d'attention somment toujours Ã  1 (distribution de probabilitÃ©)

**Sortie attendue :**
```
Sentence: The cat sleeps well
Head 1: Attention weights from 'cat' â†’ 'sleeps': 0.42
Head 2: Attention weights from 'cat' â†’ 'The': 0.38
```

---

### ğŸ“Œ Script 03 : TempÃ©rature et Softmax
**Fichier :** `03_temperature_softmax.py` | **Chapitres :** 7, 11

**Ce que fait le script :**
- Applique softmax avec diffÃ©rentes tempÃ©ratures (0.1, 0.5, 1.0, 2.0)
- Calcule l'entropie de Shannon pour chaque distribution
- GÃ©nÃ¨re des graphiques (si matplotlib est installÃ©)

**Ce que vous apprenez :**
- T < 1 : distribution "pointue" â†’ gÃ©nÃ©ration dÃ©terministe (greedy)
- T > 1 : distribution "plate" â†’ gÃ©nÃ©ration crÃ©ative/diverse
- L'entropie augmente avec la tempÃ©rature (plus d'incertitude)

**Sortie attendue :**
```
Temperature 0.5: Token 'Paris' = 85% (sharp, deterministic)
Temperature 2.0: Token 'Paris' = 35% (flat, creative)
```

---

### ğŸ“Œ Script 04 : RAG Minimal
**Fichier :** `04_rag_minimal.py` | **Chapitre :** 13

**Ce que fait le script :**
- CrÃ©e une mini base de connaissances (7 documents sur les LLM)
- Vectorise les documents avec TF-IDF
- Effectue une recherche par similaritÃ© cosinus
- Simule la gÃ©nÃ©ration augmentÃ©e par le contexte rÃ©cupÃ©rÃ©

**Ce que vous apprenez :**
- Le pipeline RAG complet : Retrieval â†’ Augmentation â†’ Generation
- Comment la similaritÃ© cosinus trouve les documents pertinents
- Pourquoi RAG permet de rÃ©pondre Ã  des questions sur des donnÃ©es privÃ©es

**Sortie attendue :**
```
Question: "Comment fonctionne l'attention dans le Transformer?"
â†’ Documents rÃ©cupÃ©rÃ©s: [doc_1: 0.72, doc_4: 0.65]
â†’ RÃ©ponse gÃ©nÃ©rÃ©e avec contexte
```

---

### ğŸ“Œ Script 05 : Ã‰valuation Pass@k
**Fichier :** `05_pass_at_k_evaluation.py` | **Chapitre :** 12

**Ce que fait le script :**
- Simule 100 tentatives de gÃ©nÃ©ration avec un taux de succÃ¨s de 30%
- Calcule Pass@k (au moins 1 succÃ¨s sur k essais)
- Calcule Pass^k (tous les k essais rÃ©ussissent)

**Ce que vous apprenez :**
- Pass@k = 1 - (1-p)^k : probabilitÃ© d'au moins un succÃ¨s
- Pass^k = p^k : probabilitÃ© que tous rÃ©ussissent (trÃ¨s strict)
- Pourquoi Pass@10 â‰ˆ 97% mÃªme avec p=30% (on a 10 chances)

**Sortie attendue :**
```
Pass@1  = 30%  (chance avec 1 essai)
Pass@5  = 83%  (chance avec 5 essais)
Pass@10 = 97%  (quasi-certain avec 10 essais)
```

---

### ğŸ Script 06 : Agent ReAct (BONUS)
**Fichier :** `06_react_agent_bonus.py` | **Chapitres :** 14, 15

**Ce que fait le script :**
- ImplÃ©mente un mini-framework d'agent autonome
- DÃ©montre la boucle ReAct : Thought â†’ Action â†’ Observation â†’ ...
- Inclut des outils simulÃ©s : calculatrice, recherche web, mÃ©tÃ©o

**Ce que vous apprenez :**
- Le pattern ReAct (Reasoning + Acting)
- Comment un agent dÃ©cide quelle action prendre
- L'auto-correction : l'agent peut rÃ©essayer si une action Ã©choue
- La base pour comprendre les agents MCP (Model Context Protocol)

**Sortie attendue :**
```
Thought: Je dois calculer 15% de 250â‚¬
Action: calculator(250 * 0.15)
Observation: 37.5
Final Answer: Le pourboire est de 37,50â‚¬
```

---

### ğŸ Script 07 : RAG AvancÃ© avec LlamaIndex (BONUS)
**Fichier :** `07_llamaindex_rag_advanced.py` | **Chapitres :** 13, 14

**Ce que fait le script :**
- SystÃ¨me RAG complet avec parsing de documents
- Indexation et embeddings (simulÃ©s ou rÃ©els avec OpenAI)
- Chat avec mÃ©moire conversationnelle
- Ã‰valuation de qualitÃ© (Precision, Recall, F1)

**Ce que vous apprenez :**
- Architecture RAG production : ingestion â†’ indexation â†’ retrieval â†’ gÃ©nÃ©ration
- Comment maintenir le contexte sur plusieurs tours de conversation
- Comment Ã©valuer la qualitÃ© d'un systÃ¨me RAG

**Sortie attendue :**
```
[Chat Mode]
User: Qu'est-ce qu'un Transformer?
Assistant: [Contexte: 3 documents] Un Transformer est...
User: Et l'attention multi-tÃªtes?
Assistant: [MÃ©moire: question prÃ©cÃ©dente + 2 docs] ...
```

---

### ğŸ Script 08 : Fine-tuning LoRA/QLoRA (BONUS)
**Fichier :** `08_lora_finetuning_example.py` | **Chapitres :** 9, 10

**Ce que fait le script :**
- Compare Full Fine-tuning vs LoRA vs QLoRA (calculs numÃ©riques)
- Affiche les Ã©conomies de VRAM et de paramÃ¨tres entraÃ®nables
- Cas d'usage : adapter LLaMA-7B pour un domaine mÃ©tier (ferroviaire)

**Ce que vous apprenez :**
- LoRA : ajoute ~0.1% de paramÃ¨tres vs fine-tuning complet
- QLoRA : quantification 4-bit + LoRA = GPU 24GB au lieu de 140GB
- Pourquoi le fine-tuning efficace dÃ©mocratise les LLM

**Sortie attendue :**
```
LLaMA-7B:
  Full Fine-tuning: 28 GB VRAM, 7B params
  LoRA (rank=8):    8 GB VRAM, 4.2M params (0.06%)
  QLoRA:            6 GB VRAM, 4.2M params + 4-bit base
```

---

### ï¿½ Script 10 : Activation Steering & 3SO (BONUS)
**Fichier :** `10_activation_steering_demo.py` | **Chapitre :** 10

**Ce que fait le script :**
- DÃ©montre le pilotage par activations (steering) : injection de vecteurs de concept
- ImplÃ©mente l'extraction de vecteurs par activation contrastive
- Simule un Sparse Autoencoder (SAE) pour la dÃ©composition en concepts
- ImplÃ©mente une machine Ã  Ã©tats finis pour le 3SO (sorties JSON garanties)
- Compare RLHF/DPO vs Steering avec tableau dÃ©taillÃ©

**Ce que vous apprenez :**
- Le steering modifie les activations Ã  l'infÃ©rence : $X_{steered} = X + (c \times V)$
- Comment extraire des vecteurs de concept (mÃ©thode contrastive, SAE)
- L'impact du coefficient de pilotage (trop faible â†’ nul, optimal â†’ efficace, trop fort â†’ dÃ©raillement)
- Le 3SO garantit mathÃ©matiquement une syntaxe JSON valide
- Quand utiliser l'alignement vs le steering

**Sortie attendue :**
```
STEP 3: Analyzing Coefficient Effect
   Coeff   Direction Î”     Perturbation    Stability
   1.0     12.5Â°           8.2%            âœ… stable
   5.0     45.3Â°           35.1%           âš ï¸ moderate
   15.0    78.2Â°           89.4%           âŒ unstable
```

---

### ï¿½ğŸ† Script 09 : Mini-Assistant Complet (PROJET FINAL)
**Fichier :** `09_mini_assistant_complet.py` | **Chapitres :** 11-15

**Ce que fait le script :**
- IntÃ¨gre TOUS les concepts : RAG + Agents + TempÃ©rature + Ã‰valuation
- SystÃ¨me complet avec base de connaissances, retrieval, raisonnement
- Mode interactif pour tester diffÃ©rentes questions

**Ce que vous apprenez :**
- Comment assembler un assistant IA complet de A Ã  Z
- L'architecture en couches : Data â†’ Retrieval â†’ Reasoning â†’ Generation
- L'Ã©valuation de bout en bout d'un systÃ¨me

**Documentation dÃ©diÃ©e :**
- [INDEX_SCRIPT_09.md](INDEX_SCRIPT_09.md) : Architecture complÃ¨te
- [QUICKSTART_SCRIPT_09.md](QUICKSTART_SCRIPT_09.md) : DÃ©marrage en 5 min
- [SCRIPT_09_MAPPING.md](SCRIPT_09_MAPPING.md) : Mapping code â†” concepts

---

## ï¿½ğŸš€ DÃ©marrage Rapide

### 1. CrÃ©er un environnement virtuel (recommandÃ©)

```bash
# Sur Windows
python -m venv venv
venv\Scripts\activate

# Sur macOS / Linux
python -m venv venv
source venv/bin/activate
```

### 2. Installer les dÃ©pendances

```bash
# Installation basique (pour les scripts 1-5)
pip install torch transformers numpy scikit-learn

# Installation complÃ¨te (avec visualisations)
pip install torch transformers numpy scikit-learn matplotlib

# Pour les bonus (optionnel, scripts fonctionnent aussi sans)
pip install llama-index openai python-dotenv peft bitsandbytes
```

**Note:** Les scripts bonus (06, 07, 08) fonctionnent **sans dÃ©pendances externes** en mode dÃ©mo.

### 3. ExÃ©cuter un script

```bash
python 01_tokenization_embeddings.py
python 02_multihead_attention.py
python 03_temperature_softmax.py
python 04_rag_minimal.py
python 05_pass_at_k_evaluation.py
python 06_react_agent_bonus.py
python 07_llamaindex_rag_advanced.py
python 08_lora_finetuning_example.py
python 09_mini_assistant_complet.py    # â† Projet intÃ©grateur final
```

---

## ğŸ† Projet IntÃ©grateur : Mini-Assistant Complet

**LE script phare** : intÃ¨gre TOUS les concepts des chapitres 11-15.

- **Script :** `09_mini_assistant_complet.py`
- **Documentation :** [INDEX_SCRIPT_09.md](INDEX_SCRIPT_09.md)
- **DÃ©marrage rapide :** [QUICKSTART_SCRIPT_09.md](QUICKSTART_SCRIPT_09.md)
- **Architecture :** [SCRIPT_09_MAPPING.md](SCRIPT_09_MAPPING.md)

---

## ğŸ“– Documentation ComplÃ¨te

- **[Parcours PÃ©dagogique](PEDAGOGICAL_JOURNEY.md)** : Correspondance chapitre par chapitre livre â†” scripts
- **[ReAct Agents](REACT_AGENT_INTEGRATION.md)** : Pattern ReAct et intÃ©gration
- **[LlamaIndex RAG](LLAMAINDEX_GUIDE.md)** : Framework RAG avancÃ©

---

## ğŸ“ Notes

- **Pas de GPU requis** : tous les scripts tournent sur CPU (plus lentement)
- **Code Ã©ducatif** : privilÃ©gient la clartÃ© sur l'optimisation
- **Compatible Python 3.9+**

---

**Bon apprentissage! ğŸš€**
