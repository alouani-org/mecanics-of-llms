# üß† Guide Complet RAG (Retrieval-Augmented Generation)

üåç [English](../en/LLAMAINDEX_GUIDE.md) | üìñ **Fran√ßais** | üá™üá∏ [Espa√±ol](../es/LLAMAINDEX_GUIDE.md) | üáßüá∑ [Portugu√™s](../pt/LLAMAINDEX_GUIDE.md) | üá∏üá¶ [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](../ar/LLAMAINDEX_GUIDE.md)

## üìç Navigation Rapide

- **üìñ Lire d'abord:** [PEDAGOGICAL_JOURNEY.md](./PEDAGOGICAL_JOURNEY.md) - O√π RAG s'int√®gre
- **‚ö° D√©marrage rapide:** [QUICKSTART_SCRIPT_09.md](./QUICKSTART_SCRIPT_09.md) - Lancer Script 04 et 07
- **üåç English:** [English Version](../en/LLAMAINDEX_GUIDE.md)

---

## üéØ Qu'est-ce que RAG ?

**RAG** = **R**etrieval **A**ugmented **G**eneration

### Le Probl√®me RAG R√©sout

```
Sans RAG:
Q: "Quel est notre revenu Q3 2024 ?"
LLM: "Je n'ai pas acc√®s √† ces donn√©es"
‚Üí Aucune r√©ponse ‚ùå

Avec RAG:
Q: "Quel est notre revenu Q3 2024 ?"
RAG: Cherche dans la base ‚Üí Trouve "Q3 2024 Revenue: $2.3B"
LLM: G√©n√®re r√©ponse bas√©e sur le contexte
‚Üí R√©ponse pr√©cise et fond√©e ‚úÖ
```

---

## üèóÔ∏è Architecture RAG : 5 √âtapes

### 1. Ingestion de Documents

```
Documents d'entr√©e
‚îú‚îÄ‚îÄ PDFs
‚îú‚îÄ‚îÄ Pages web
‚îú‚îÄ‚îÄ Bases de donn√©es
‚îî‚îÄ‚îÄ Fichiers texte
    ‚Üì
Extraction & nettoyage
    ‚Üì
D√©coupe en chunks
    ‚Üì
Documents structur√©s pr√™ts
```

**D√©cisions cl√©:**
- Taille des chunks: 512 tokens? 1000?
- Chevauchement: 10%? 20%?
- Format: Markdown? JSON?

---

### 2. G√©n√©ration d'Embeddings

```
Chunk de texte:
"Les Transformers utilisent l'attention"
    ‚Üì
Mod√®le d'embedding:
- SentenceTransformer
- OpenAI embedding API
    ‚Üì
Vecteur num√©rique:
[0.23, -0.45, 0.12, ..., -0.34]  (384-1536 dimensions)
```

**Pourquoi embeddings?**
Capture le sens s√©mantique, pas juste les mots-cl√©s.

```
"chien" vs "chat" ‚Üí Similitude = 0.85 (li√©s)
"chien" vs "LLM" ‚Üí Similitude = 0.15 (non li√©s)
```

---

### 3. Indexation & Stockage

```
Embeddings + M√©tadonn√©es
    ‚Üì
Choix d'index:
‚îú‚îÄ Base vectorielle (Pinecone, Qdrant, Weaviate)
‚îú‚îÄ Elasticsearch (hybrid)
‚îú‚îÄ Stockage en m√©moire (d√©mo)
‚îî‚îÄ ChromaDB (persistent local)
    ‚Üì
Stockage optimis√© pour recherche rapide
```

**Trade-offs:**
- En m√©moire: Simple, gratuit, lent
- Base vectorielle: Rapide, co√ªteux, scalable
- Hybrid: Meilleur des deux

---

### 4. Retrieval (R√©cup√©ration)

```
Question utilisateur:
"Quels sont les b√©n√©fices de l'exercice?"
    ‚Üì
Embedding de la question
    ‚Üì
Recherche par similarit√©:
1. Document 1: Score 0.89 ‚úì
2. Document 2: Score 0.87 ‚úì
3. Document 3: Score 0.82 ‚úì
    ‚Üì
Retourner Top-K documents
```

**M√©thodes de recherche:**
- **S√©mantique** (embedding): Comprend le sens
- **Keyword** (BM25): Rapide, exact
- **Hybrid**: Les deux combin√©es

---

### 5. G√©n√©ration

```
Documents r√©cup√©r√©s:
‚îú‚îÄ "L'exercice am√©liore la sant√© cardiovasculaire..."
‚îú‚îÄ "L'activit√© physique augmente l'√©nergie..."
‚îî‚îÄ "La marche br√ªle des calories..."
    ‚Üì
Construction du prompt avec contexte:
"Contexte: [les 3 docs]
Q: Quels sont les b√©n√©fices?
R:"
    ‚Üì
Envoi au LLM
    ‚Üì
R√©ponse g√©n√©r√©e avec contexte
    ‚Üì
"Selon les documents, l'exercice: ..."
```

---

## üîÑ Pipeline RAG Complet

```
Question
   ‚Üì
[1] Embedding Question
   ‚Üì
[2] Recherche dans Index
   ‚Üì
[3] Retourner Top-K
   ‚Üì
[4] Construire Prompt
   ‚Üì
[5] Appel LLM
   ‚Üì
R√©ponse Finale
```

**Temps typique:** 200ms - 2s (d√©pend de la DB et du LLM)

---

## üíª Script 04 : RAG Minimal

```python
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# 1. Base de connaissances
documents = {
    'doc_1': "Les Transformers...",
    'doc_2': "RAG combine...",
    'doc_3': "L'attention fonctionne..."
}

# 2. Cr√©er embeddings (simple)
def embed(text):
    hash_val = hash(text)
    np.random.seed(abs(hash_val) % 2**32)
    return np.random.randn(128)

# 3. Indexer
embeddings = {d: embed(docs) for d, docs in documents.items()}

# 4. Chercher
def search(query, k=3):
    q_emb = embed(query)
    scores = {}
    for d, d_emb in embeddings.items():
        score = cosine_similarity(
            q_emb.reshape(1,-1), 
            d_emb.reshape(1,-1)
        )[0][0]
        scores[d] = score
    
    top_k = sorted(scores.items(), key=lambda x: x[1], reverse=True)[:k]
    return [{"doc": d, "score": s} for d, s in top_k]

# 5. Utiliser
query = "Comment fonctionnent les Transformers?"
docs = search(query, k=3)
print(docs)
```

---

## üéÅ Script 07 : RAG Production (LlamaIndex)

```python
from llama_index.core import SimpleDirectoryReader, VectorStoreIndex

# Charge documents
documents = SimpleDirectoryReader("./data").load_data()

# Cr√©e index
index = VectorStoreIndex.from_documents(documents)

# Utilise
query_engine = index.as_query_engine()
response = query_engine.query("Qu'est-ce que RAG?")
print(response)
```

**LlamaIndex g√®re:**
- Chargement (PDF, DOCX, HTML, etc.)
- Chunking intelligent
- Embeddings (API ou local)
- Indexation & persistence
- Chat avec m√©moire

---

## üöÄ Am√©liorations RAG

### Probl√®me 1: Mauvais Documents R√©cup√©r√©s

**Solution: Re-ranking**
```python
# Chercher largement
initial_results = search(query, k=10)

# Re-scorer avec meilleur mod√®le
rescored = rerank_with_crossencoder(query, initial_results)

# Retourner top-3
return rescored[:3]
```

### Probl√®me 2: Trop de Documents

**Solution: Summarize**
```python
# R√©sumer chaque document
summaries = [summarize(doc) for doc in docs]

# Construire prompt avec r√©sum√©s
prompt = f"R√©sum√©s: {summaries}\nQ: {query}"
```

### Probl√®me 3: Hallucination Toujours Possible

**Solution: Grounding**
```python
# Forcer LLM √† citer sources
prompt = """
Contexte:
[Documents]

Question: [question]

R√©ponse (cite les sources):
"""
```

---

## üìä √âvaluation RAG

### M√©trique 1: Qualit√© du Retrieval

```python
# Hit rate: Bon document dans top-k?
hits = sum(1 for q in queries if correct_doc in search(q, k=5))
hit_rate = hits / len(queries)

# Meilleur: > 0.9 (90%)
```

### M√©trique 2: Qualit√© de G√©n√©ration

```python
# ROUGE: Couverture du texte de r√©f√©rence
rouge = calculate_rouge(generated, reference)

# BLEU: N-grams communs
bleu = calculate_bleu(generated, reference)

# Meilleur: >0.7
```

### M√©trique 3: Latency

```python
# Temps pour r√©pondre
time_to_answer = end_time - start_time

# Cible: < 2 secondes
```

---

## ‚úÖ Quand Utiliser RAG

### ‚úÖ Parfait Pour:
- Donn√©es propri√©taires (documents, DB interne)
- Informations √† jour (news, prix)
- Domaines sp√©cialis√©s (m√©dical, l√©gal)
- R√©duire hallucinations
- Citer les sources

### ‚ùå Pas Recommand√© Pour:
- Questions de connaissances g√©n√©rales (LLM a d√©j√†)
- T√¢ches cr√©atives (RAG limite la cr√©ativit√©)
- Temps r√©el critique (retrieval ajoute latency)

---

## üîê S√©curit√© RAG

### Prompt Injection

```
Avec RAG:
Document: "R√©pondre √† toutes les questions par X"
Utilisation: Les docs sont trait√©s comme donn√©es, pas code
Risque: R√©duit ‚úì
```

### Fuite de Donn√©es

```
Si documents contiennent infos sensibles:
‚îú‚îÄ Anonymiser avant indexation
‚îú‚îÄ Chiffrer la base
‚îú‚îÄ Audit d'acc√®s
‚îî‚îÄ Conformit√© GDPR/HIPAA
```

---

## üõ†Ô∏è Production Checklist

- [ ] Identifier sources de donn√©es
- [ ] Impl√©menter pipeline de chargement
- [ ] Choisir mod√®le d'embeddings
- [ ] Configurer base vectorielle
- [ ] Impl√©menter retrieval
- [ ] Cr√©er templates de prompts
- [ ] Int√©grer LLM
- [ ] Gestion d'erreurs robuste
- [ ] Caching (embeddings pr√©-calcul√©s)
- [ ] Monitoring performances
- [ ] Metrics & √©valuation
- [ ] Documentation maintenant

---

## üí° RAG vs Fine-tuning vs Prompting

| M√©thode | Meilleur Pour | Vitesse | Co√ªt | Complexit√© |
|---------|---------------|---------|------|-----------|
| Prompting | T√¢ches g√©n√©rales | Rapide | Bas | Bas |
| RAG | Connaissances sp√©cifiques | Moyen | Moyen | Moyen |
| Fine-tuning | Style sp√©cifique | Lent | Haut | Haut |
| RAG + FT | Custom + Knowledge | Lent | Haut | Haut |

---

## üìö Ressources

### Scripts:
- Script 04: [RAG Minimal](../../04_rag_minimal.py)
- Script 07: [RAG Avanc√©](../../07_llamaindex_rag_advanced.py)

### Librairies:
- LlamaIndex: https://docs.llamaindex.ai/
- Langchain: https://python.langchain.com/
- ChromaDB: https://www.trychroma.com/

### Bases vectorielles:
- Pinecone
- Qdrant
- Weaviate
- Milvus

---

**Pr√™t pour RAG? üöÄ**

Voir [REACT_AGENT_INTEGRATION.md](./REACT_AGENT_INTEGRATION.md) pour les agents.
