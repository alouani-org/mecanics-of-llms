# ğŸ“š Guia LlamaIndex para Iniciantes

ğŸŒ [English](../en/LLAMAINDEX_GUIDE.md) | ğŸ“– [FranÃ§ais](../fr/LLAMAINDEX_GUIDE.md) | ğŸ‡ªğŸ‡¸ [EspaÃ±ol](../es/LLAMAINDEX_GUIDE.md) | ğŸ‡§ğŸ‡· **PortuguÃªs** | ğŸ‡¸ğŸ‡¦ [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](../ar/LLAMAINDEX_GUIDE.md)

> **Construindo sistemas RAG com LlamaIndex**  
> Guia Passo a Passo

---

## ğŸ“ NavegaÃ§Ã£o RÃ¡pida

- **ğŸ“– Ver: [Jornada PedagÃ³gica](PEDAGOGICAL_JOURNEY.md)** - Onde isso se encaixa
- **âš¡ Ver: [InÃ­cio RÃ¡pido Script 09](QUICKSTART_SCRIPT_09.md)** - Usar RAG
- **ğŸ—ºï¸ Ver: [Mapa CÃ³digo â†” Conceitos](SCRIPT_09_MAPPING.md)** - Mapeamento detalhado
- **ğŸŒ Outros idiomas: [English](../en/LLAMAINDEX_GUIDE.md) | [FranÃ§ais](../fr/LLAMAINDEX_GUIDE.md) | [EspaÃ±ol](../es/LLAMAINDEX_GUIDE.md)**

---

## ğŸ¯ O que Ã© LlamaIndex?

**LlamaIndex** Ã© um framework que facilita:

1. **Carregar** seus prÃ³prios dados (PDF, texto, pÃ¡ginas web)
2. **Indexar** esses dados para busca rÃ¡pida
3. **Consultar** usando linguagem natural
4. **Sintetizar** respostas com LLMs

### Analogia

```
LlamaIndex = Seu BibliotecÃ¡rio IA

1. VocÃª dÃ¡ livros para ele (seus documentos)
2. Ele organiza (cria Ã­ndice)
3. VocÃª faz perguntas ("Onde fala sobre X?")
4. Ele encontra e resume a resposta
```

---

## ğŸ—ï¸ Arquitetura LlamaIndex

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              SEUS DOCUMENTOS                 â”‚
â”‚  (PDFs, TXTs, PÃ¡ginas Web, Bancos de Dados) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CARREGADORES (Loaders)            â”‚
â”‚  SimpleDirectoryReader, PDFReader, etc.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                NÃ“S (Nodes)                   â”‚
â”‚  Fragmentos de texto com metadados          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ÃNDICE (Index)                 â”‚
â”‚  VectorStoreIndex, TreeIndex, etc.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       MOTOR DE CONSULTA (Query Engine)      â”‚
â”‚  Recupera nÃ³s relevantes + Gera resposta    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Conceitos Chave

### 1. **Documento**

Um documento Ã© seu dado fonte:

```python
from llama_index.core import Document

# Criar documento a partir de texto
doc = Document(text="O cÃ©u Ã© azul...")

# Criar documento com metadados
doc = Document(
    text="O cÃ©u Ã© azul...",
    metadata={
        "source": "meu_arquivo.txt",
        "author": "JoÃ£o Silva",
        "date": "2024-01-15"
    }
)
```

### 2. **NÃ³**

Um nÃ³ Ã© um fragmento de documento:

```python
# Um documento grande Ã© dividido em nÃ³s
Documento: "O cÃ©u Ã© azul. O oceano Ã© profundo. As estrelas brilham."

# Torna-se nÃ³s:
NÃ³ 1: "O cÃ©u Ã© azul."
NÃ³ 2: "O oceano Ã© profundo."
NÃ³ 3: "As estrelas brilham."
```

### 3. **Ãndice**

Um Ã­ndice organiza nÃ³s para busca rÃ¡pida:

```python
from llama_index.core import VectorStoreIndex

# Criar Ã­ndice a partir de documentos
index = VectorStoreIndex.from_documents(documents)

# O Ã­ndice contÃ©m embeddings para cada nÃ³
# Isso permite busca semÃ¢ntica rÃ¡pida
```

### 4. **Motor de Consulta**

O motor de consulta responde perguntas:

```python
# Criar motor de consulta a partir do Ã­ndice
query_engine = index.as_query_engine()

# Fazer pergunta
response = query_engine.query("Qual Ã© a cor do cÃ©u?")
print(response)  # "O cÃ©u Ã© azul"
```

---

## ğŸš€ Script 09: RAG com LlamaIndex

### Passo 1: Configurar Ambiente

```python
# Importar dependÃªncias
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# Configurar LLM e Embeddings
Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.7)
Settings.embed_model = OpenAIEmbedding()
```

### Passo 2: Carregar Documentos

```python
# Carregar de diretÃ³rio
reader = SimpleDirectoryReader("./data")
documents = reader.load_data()

print(f"Carregados {len(documents)} documentos")

# Ver conteÃºdo de um documento
print(documents[0].text[:200])  # Primeiros 200 caracteres
```

### Passo 3: Criar Ãndice

```python
# Criar Ã­ndice vetorial
index = VectorStoreIndex.from_documents(documents)

# O Ã­ndice:
# 1. Divide documentos em nÃ³s (fragmentos)
# 2. Gera embeddings para cada nÃ³
# 3. Armazena em vector store
```

### Passo 4: Consultar

```python
# Criar motor de consulta
query_engine = index.as_query_engine(
    similarity_top_k=3,  # Recuperar top 3 nÃ³s relevantes
)

# Fazer pergunta
response = query_engine.query(
    "Quais sÃ£o as principais caracterÃ­sticas dos LLMs?"
)

print(response.response)

# Ver fontes utilizadas
for node in response.source_nodes:
    print(f"Fonte: {node.node.metadata.get('source', 'desconhecida')}")
    print(f"Score: {node.score:.3f}")
```

---

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Personalizar Chunking (DivisÃ£o)

```python
from llama_index.core.node_parser import SentenceSplitter

# Configurar como dividir documentos
node_parser = SentenceSplitter(
    chunk_size=1024,      # Tokens mÃ¡ximos por chunk
    chunk_overlap=200     # SobreposiÃ§Ã£o entre chunks
)

# Criar Ã­ndice com parser personalizado
index = VectorStoreIndex.from_documents(
    documents,
    node_parser=node_parser
)
```

### Personalizar Prompt

```python
from llama_index.core import PromptTemplate

# Criar prompt personalizado
template = """
Contexto: {context_str}

Baseado no contexto acima, responda a seguinte pergunta.
Se nÃ£o encontrar a informaÃ§Ã£o no contexto, diga "NÃ£o tenho informaÃ§Ã£o suficiente".

Pergunta: {query_str}

Resposta:
"""

qa_prompt = PromptTemplate(template)

# Usar no motor de consulta
query_engine = index.as_query_engine(
    text_qa_template=qa_prompt
)
```

### Persistir Ãndice

```python
# Salvar Ã­ndice em disco
index.storage_context.persist(persist_dir="./storage")

# Carregar Ã­ndice salvo
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)

# Agora nÃ£o precisa re-processar documentos!
```

---

## ğŸ“Š Tipos de Ãndice

### 1. VectorStoreIndex (Mais Comum)

```python
# Usa embeddings para busca semÃ¢ntica
index = VectorStoreIndex.from_documents(documents)

# Melhor para: Perguntas sobre conteÃºdo especÃ­fico
# "O que o documento diz sobre X?"
```

### 2. SummaryIndex

```python
from llama_index.core import SummaryIndex

# Armazena resumos de documentos
index = SummaryIndex.from_documents(documents)

# Melhor para: Perguntas que requerem visÃ£o geral
# "Resuma todo o documento"
```

### 3. TreeIndex

```python
from llama_index.core import TreeIndex

# Organiza em estrutura de Ã¡rvore
index = TreeIndex.from_documents(documents)

# Melhor para: Documentos hierÃ¡rquicos
# Livros com capÃ­tulos, manuais com seÃ§Ãµes
```

---

## âš™ï¸ ParÃ¢metros Importantes

### similarity_top_k

```python
# Quantos nÃ³s recuperar
query_engine = index.as_query_engine(similarity_top_k=5)

# k pequeno (1-3): Respostas mais focadas
# k grande (5-10): Mais contexto, mas pode incluir ruÃ­do
```

### response_mode

```python
# Como sintetizar resposta
query_engine = index.as_query_engine(
    response_mode="compact"  # OpÃ§Ãµes: refine, compact, tree_summarize
)

# "compact": Une todo contexto, gera uma resposta
# "refine": Refina resposta iterativamente com cada nÃ³
# "tree_summarize": Resume em estrutura de Ã¡rvore
```

### streaming

```python
# Habilitar streaming para respostas longas
query_engine = index.as_query_engine(streaming=True)

response = query_engine.query("Explique em detalhes...")

# Imprimir token por token
for text in response.response_gen:
    print(text, end="", flush=True)
```

---

## ğŸ” DepuraÃ§Ã£o

### Ver O Que Ã‰ Recuperado

```python
# Obter nÃ³s sem gerar resposta
retriever = index.as_retriever(similarity_top_k=3)
nodes = retriever.retrieve("O que Ã© um LLM?")

for node in nodes:
    print(f"Score: {node.score:.3f}")
    print(f"Texto: {node.node.text[:200]}...")
    print(f"Metadados: {node.node.metadata}")
    print("---")
```

### Logging Detalhado

```python
import logging
import sys

# Habilitar logging
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Agora vocÃª verÃ¡ todos os passos internos
```

---

## ğŸ¯ Melhores PrÃ¡ticas

### 1. **Preparar Dados**

```python
# âœ… Bom: Dados limpos e estruturados
documents = [
    Document(text="CapÃ­tulo 1: IntroduÃ§Ã£o...", metadata={"chapter": 1}),
    Document(text="CapÃ­tulo 2: Conceitos...", metadata={"chapter": 2}),
]

# âŒ Ruim: Dados sujos com muito ruÃ­do
documents = [
    Document(text="asdfasdf CapÃ­tulo 1 ||||| IntroduÃ§Ã£o.....")
]
```

### 2. **Ajustar Chunk Size**

```python
# Documentos tÃ©cnicos: chunks menores
node_parser = SentenceSplitter(chunk_size=512)

# Documentos narrativos: chunks maiores
node_parser = SentenceSplitter(chunk_size=2048)
```

### 3. **Usar Metadados**

```python
# Os metadados ajudam a filtrar e contextualizar
doc = Document(
    text="ConteÃºdo do relatÃ³rio financeiro Q3 2024...",
    metadata={
        "type": "financial_report",
        "quarter": "Q3",
        "year": 2024,
        "department": "financeiro"
    }
)
```

### 4. **Persistir Sempre**

```python
# NÃ£o re-processar documentos toda vez
index.storage_context.persist(persist_dir="./storage")

# Verificar se existe Ã­ndice salvo
import os
if os.path.exists("./storage"):
    index = load_index_from_storage(...)
else:
    index = VectorStoreIndex.from_documents(...)
```

---

## ğŸ› Erros Comuns

### Erro: "Rate limit exceeded"

```python
# Problema: Muitas chamadas API

# SoluÃ§Ã£o 1: Reduzir concorrÃªncia
Settings.num_workers = 1

# SoluÃ§Ã£o 2: Adicionar delays
import time
time.sleep(1)  # Entre operaÃ§Ãµes
```

### Erro: "Context length exceeded"

```python
# Problema: Documento muito grande

# SoluÃ§Ã£o: Reduzir chunk_size
node_parser = SentenceSplitter(chunk_size=256)
```

### Erro: "Empty response"

```python
# Problema: NÃ£o encontrou informaÃ§Ã£o relevante

# SoluÃ§Ã£o 1: Aumentar similarity_top_k
query_engine = index.as_query_engine(similarity_top_k=10)

# SoluÃ§Ã£o 2: Verificar que os documentos contÃªm a informaÃ§Ã£o
```

---

## ğŸ“š Script 09: IntegraÃ§Ã£o Completa

Script 09 combina tudo que aprendemos:

```python
# 1. Carrega documentos
documents = SimpleDirectoryReader("./data").load_data()

# 2. Cria Ã­ndice com configuraÃ§Ã£o Ã³tima
index = VectorStoreIndex.from_documents(
    documents,
    show_progress=True
)

# 3. Motor de consulta configurado
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact"
)

# 4. Ciclo interativo
while True:
    question = input("Pergunta: ")
    if question.lower() == "sair":
        break
    
    response = query_engine.query(question)
    print(f"\nResposta: {response}")
    print(f"Fontes: {len(response.source_nodes)}")
```

---

## ğŸ¯ Resumo

| Conceito | LlamaIndex | FunÃ§Ã£o |
|----------|------------|--------|
| **Document** | `Document` | Seu dado fonte |
| **Node** | Fragmento | PedaÃ§o de documento |
| **Index** | `VectorStoreIndex` | Organiza nÃ³s |
| **Query Engine** | `as_query_engine()` | Responde perguntas |
| **Retriever** | `as_retriever()` | Busca nÃ³s relevantes |

---

## ğŸš€ PrÃ³ximos Passos

1. âœ… Execute Script 09 com seus prÃ³prios documentos
2. âœ… Experimente com diferentes `chunk_size`
3. âœ… Teste diferentes `response_mode`
4. âœ… Adicione metadados aos seus documentos
5. âœ… Persista seu Ã­ndice

---

**Pronto para construir seu prÃ³prio sistema RAG? ğŸš€**

Experimente o Script 09 agora!
