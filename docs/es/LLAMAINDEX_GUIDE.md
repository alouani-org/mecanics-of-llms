# ğŸ“š GuÃ­a LlamaIndex para Principiantes

ğŸŒ [English](../en/LLAMAINDEX_GUIDE.md) | ğŸ“– [FranÃ§ais](../fr/LLAMAINDEX_GUIDE.md) | ğŸ‡ªğŸ‡¸ **EspaÃ±ol** | ğŸ‡§ğŸ‡· [PortuguÃªs](../pt/LLAMAINDEX_GUIDE.md) | ğŸ‡¸ğŸ‡¦ [Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©](../ar/LLAMAINDEX_GUIDE.md)

> **Construyendo sistemas RAG con LlamaIndex**  
> GuÃ­a Paso a Paso

---

## ğŸ“ NavegaciÃ³n RÃ¡pida

- **ğŸ“– Ver: [Recorrido PedagÃ³gico](PEDAGOGICAL_JOURNEY.md)** - DÃ³nde encaja esto
- **âš¡ Ver: [Inicio RÃ¡pido Script 09](QUICKSTART_SCRIPT_09.md)** - Usar RAG
- **ğŸ—ºï¸ Ver: [Mapa CÃ³digo â†” Conceptos](SCRIPT_09_MAPPING.md)** - Mapeo detallado
- **ğŸŒ Otros idiomas: [English](../en/LLAMAINDEX_GUIDE.md) | [FranÃ§ais](../fr/LLAMAINDEX_GUIDE.md) | [PortuguÃªs](../pt/LLAMAINDEX_GUIDE.md)**

---

## ğŸ¯ Â¿QuÃ© es LlamaIndex?

**LlamaIndex** es un framework que facilita:

1. **Cargar** tus propios datos (PDF, texto, pÃ¡ginas web)
2. **Indexar** esos datos para bÃºsqueda rÃ¡pida
3. **Consultar** usando lenguaje natural
4. **Sintetizar** respuestas con LLMs

### AnalogÃ­a

```
LlamaIndex = Tu Bibliotecario IA

1. TÃº le das libros (tus documentos)
2. Ã‰l los organiza (crea Ã­ndice)
3. TÃº haces preguntas ("Â¿DÃ³nde habla de X?")
4. Ã‰l encuentra y resume la respuesta
```

---

## ğŸ—ï¸ Arquitectura LlamaIndex

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              TUS DOCUMENTOS                  â”‚
â”‚  (PDFs, TXTs, PÃ¡ginas Web, Bases de Datos)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            CARGADORES (Loaders)              â”‚
â”‚  SimpleDirectoryReader, PDFReader, etc.     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              NODOS (Nodes)                   â”‚
â”‚  Fragmentos de texto con metadatos          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ÃNDICE (Index)                  â”‚
â”‚  VectorStoreIndex, TreeIndex, etc.          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MOTOR DE CONSULTA (Query Engine)    â”‚
â”‚  Recupera nodos relevantes + Genera resp.   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Conceptos Clave

### 1. **Documento**

Un documento es tu dato fuente:

```python
from llama_index.core import Document

# Crear documento desde texto
doc = Document(text="El cielo es azul...")

# Crear documento con metadatos
doc = Document(
    text="El cielo es azul...",
    metadata={
        "source": "mi_archivo.txt",
        "author": "Juan PÃ©rez",
        "date": "2024-01-15"
    }
)
```

### 2. **Nodo**

Un nodo es un fragmento de documento:

```python
# Un documento grande se divide en nodos
Documento: "El cielo es azul. El ocÃ©ano es profundo. Las estrellas brillan."

# Se convierte en nodos:
Nodo 1: "El cielo es azul."
Nodo 2: "El ocÃ©ano es profundo."
Nodo 3: "Las estrellas brillan."
```

### 3. **Ãndice**

Un Ã­ndice organiza nodos para bÃºsqueda rÃ¡pida:

```python
from llama_index.core import VectorStoreIndex

# Crear Ã­ndice desde documentos
index = VectorStoreIndex.from_documents(documents)

# El Ã­ndice contiene embeddings para cada nodo
# Esto permite bÃºsqueda semÃ¡ntica rÃ¡pida
```

### 4. **Motor de Consulta**

El motor de consulta responde preguntas:

```python
# Crear motor de consulta desde Ã­ndice
query_engine = index.as_query_engine()

# Hacer pregunta
response = query_engine.query("Â¿De quÃ© color es el cielo?")
print(response)  # "El cielo es azul"
```

---

## ğŸš€ Script 09: RAG con LlamaIndex

### Paso 1: Configurar Ambiente

```python
# Importar dependencias
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings
)
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# Configurar LLM y Embeddings
Settings.llm = OpenAI(model="gpt-3.5-turbo", temperature=0.7)
Settings.embed_model = OpenAIEmbedding()
```

### Paso 2: Cargar Documentos

```python
# Cargar desde directorio
reader = SimpleDirectoryReader("./data")
documents = reader.load_data()

print(f"Cargados {len(documents)} documentos")

# Ver contenido de un documento
print(documents[0].text[:200])  # Primeros 200 caracteres
```

### Paso 3: Crear Ãndice

```python
# Crear Ã­ndice vectorial
index = VectorStoreIndex.from_documents(documents)

# El Ã­ndice:
# 1. Divide documentos en nodos (fragmentos)
# 2. Genera embeddings para cada nodo
# 3. Almacena en vector store
```

### Paso 4: Consultar

```python
# Crear motor de consulta
query_engine = index.as_query_engine(
    similarity_top_k=3,  # Recuperar top 3 nodos relevantes
)

# Hacer pregunta
response = query_engine.query(
    "Â¿CuÃ¡les son las principales caracterÃ­sticas de los LLMs?"
)

print(response.response)

# Ver fuentes utilizadas
for node in response.source_nodes:
    print(f"Fuente: {node.node.metadata.get('source', 'desconocida')}")
    print(f"Score: {node.score:.3f}")
```

---

## ğŸ”§ ConfiguraciÃ³n Avanzada

### Personalizar Chunking (DivisiÃ³n)

```python
from llama_index.core.node_parser import SentenceSplitter

# Configurar cÃ³mo dividir documentos
node_parser = SentenceSplitter(
    chunk_size=1024,      # Tokens mÃ¡ximos por chunk
    chunk_overlap=200     # SuperposiciÃ³n entre chunks
)

# Crear Ã­ndice con parser personalizado
index = VectorStoreIndex.from_documents(
    documents,
    node_parser=node_parser
)
```

### Personalizar Prompt

```python
from llama_index.core import PromptTemplate

# Crear prompt personalizado
template = """
Contexto: {context_str}

BasÃ¡ndote en el contexto anterior, responde la siguiente pregunta.
Si no encuentras la informaciÃ³n en el contexto, di "No tengo informaciÃ³n suficiente".

Pregunta: {query_str}

Respuesta:
"""

qa_prompt = PromptTemplate(template)

# Usar en motor de consulta
query_engine = index.as_query_engine(
    text_qa_template=qa_prompt
)
```

### Persistir Ãndice

```python
# Guardar Ã­ndice en disco
index.storage_context.persist(persist_dir="./storage")

# Cargar Ã­ndice guardado
from llama_index.core import StorageContext, load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)

# Â¡Ahora no necesitas re-procesar documentos!
```

---

## ğŸ“Š Tipos de Ãndice

### 1. VectorStoreIndex (MÃ¡s ComÃºn)

```python
# Usa embeddings para bÃºsqueda semÃ¡ntica
index = VectorStoreIndex.from_documents(documents)

# Mejor para: Preguntas sobre contenido especÃ­fico
# "Â¿QuÃ© dice el documento sobre X?"
```

### 2. SummaryIndex

```python
from llama_index.core import SummaryIndex

# Almacena resÃºmenes de documentos
index = SummaryIndex.from_documents(documents)

# Mejor para: Preguntas que requieren visiÃ³n general
# "Resume todo el documento"
```

### 3. TreeIndex

```python
from llama_index.core import TreeIndex

# Organiza en estructura de Ã¡rbol
index = TreeIndex.from_documents(documents)

# Mejor para: Documentos jerÃ¡rquicos
# Libros con capÃ­tulos, manuales con secciones
```

---

## âš™ï¸ ParÃ¡metros Importantes

### similarity_top_k

```python
# CuÃ¡ntos nodos recuperar
query_engine = index.as_query_engine(similarity_top_k=5)

# k pequeÃ±o (1-3): Respuestas mÃ¡s enfocadas
# k grande (5-10): MÃ¡s contexto, pero puede incluir ruido
```

### response_mode

```python
# CÃ³mo sintetizar respuesta
query_engine = index.as_query_engine(
    response_mode="compact"  # Opciones: refine, compact, tree_summarize
)

# "compact": Une todo contexto, genera una respuesta
# "refine": Refina respuesta iterativamente con cada nodo
# "tree_summarize": Resume en estructura de Ã¡rbol
```

### streaming

```python
# Habilitar streaming para respuestas largas
query_engine = index.as_query_engine(streaming=True)

response = query_engine.query("Explica en detalle...")

# Imprimir token por token
for text in response.response_gen:
    print(text, end="", flush=True)
```

---

## ğŸ” DepuraciÃ³n

### Ver QuÃ© Se Recupera

```python
# Obtener nodos sin generar respuesta
retriever = index.as_retriever(similarity_top_k=3)
nodes = retriever.retrieve("Â¿QuÃ© es un LLM?")

for node in nodes:
    print(f"Score: {node.score:.3f}")
    print(f"Texto: {node.node.text[:200]}...")
    print(f"Metadatos: {node.node.metadata}")
    print("---")
```

### Logging Detallado

```python
import logging
import sys

# Habilitar logging
logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

# Ahora verÃ¡s todos los pasos internos
```

---

## ğŸ¯ Mejores PrÃ¡cticas

### 1. **Preparar Datos**

```python
# âœ… Bueno: Datos limpios y estructurados
documents = [
    Document(text="CapÃ­tulo 1: IntroducciÃ³n...", metadata={"chapter": 1}),
    Document(text="CapÃ­tulo 2: Conceptos...", metadata={"chapter": 2}),
]

# âŒ Malo: Datos sucios con mucho ruido
documents = [
    Document(text="asdfasdf CapÃ­tulo 1 ||||| IntroducciÃ³n.....")
]
```

### 2. **Ajustar Chunk Size**

```python
# Documentos tÃ©cnicos: chunks mÃ¡s pequeÃ±os
node_parser = SentenceSplitter(chunk_size=512)

# Documentos narrativos: chunks mÃ¡s grandes
node_parser = SentenceSplitter(chunk_size=2048)
```

### 3. **Usar Metadatos**

```python
# Los metadatos ayudan a filtrar y contextualizar
doc = Document(
    text="Contenido del informe financiero Q3 2024...",
    metadata={
        "type": "financial_report",
        "quarter": "Q3",
        "year": 2024,
        "department": "finanzas"
    }
)
```

### 4. **Persistir Siempre**

```python
# No re-procesar documentos cada vez
index.storage_context.persist(persist_dir="./storage")

# Verificar si existe Ã­ndice guardado
import os
if os.path.exists("./storage"):
    index = load_index_from_storage(...)
else:
    index = VectorStoreIndex.from_documents(...)
```

---

## ğŸ› Errores Comunes

### Error: "Rate limit exceeded"

```python
# Problema: Demasiadas llamadas API

# SoluciÃ³n 1: Reducir concurrencia
Settings.num_workers = 1

# SoluciÃ³n 2: AÃ±adir delays
import time
time.sleep(1)  # Entre operaciones
```

### Error: "Context length exceeded"

```python
# Problema: Documento muy grande

# SoluciÃ³n: Reducir chunk_size
node_parser = SentenceSplitter(chunk_size=256)
```

### Error: "Empty response"

```python
# Problema: No se encontrÃ³ informaciÃ³n relevante

# SoluciÃ³n 1: Aumentar similarity_top_k
query_engine = index.as_query_engine(similarity_top_k=10)

# SoluciÃ³n 2: Verificar que los documentos contienen la informaciÃ³n
```

---

## ğŸ“š Script 09: IntegraciÃ³n Completa

Script 09 combina todo lo aprendido:

```python
# 1. Carga documentos
documents = SimpleDirectoryReader("./data").load_data()

# 2. Crea Ã­ndice con configuraciÃ³n Ã³ptima
index = VectorStoreIndex.from_documents(
    documents,
    show_progress=True
)

# 3. Motor de consulta configurado
query_engine = index.as_query_engine(
    similarity_top_k=3,
    response_mode="compact"
)

# 4. Ciclo interactivo
while True:
    question = input("Pregunta: ")
    if question.lower() == "salir":
        break
    
    response = query_engine.query(question)
    print(f"\nRespuesta: {response}")
    print(f"Fuentes: {len(response.source_nodes)}")
```

---

## ğŸ¯ Resumen

| Concepto | LlamaIndex | FunciÃ³n |
|----------|------------|---------|
| **Document** | `Document` | Tu dato fuente |
| **Node** | Fragmento | Pieza de documento |
| **Index** | `VectorStoreIndex` | Organiza nodos |
| **Query Engine** | `as_query_engine()` | Responde preguntas |
| **Retriever** | `as_retriever()` | Busca nodos relevantes |

---

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Ejecuta Script 09 con tus propios documentos
2. âœ… Experimenta con diferentes `chunk_size`
3. âœ… Prueba diferentes `response_mode`
4. âœ… AÃ±ade metadatos a tus documentos
5. âœ… Persiste tu Ã­ndice

---

**Â¿Listo para construir tu propio sistema RAG? ğŸš€**

Â¡Prueba Script 09 ahora!
