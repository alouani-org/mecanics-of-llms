# Scripts Pratiques : Exp√©rimenter les Concepts LLM

Ce dossier regroupe des **micro-scripts Python ex√©cutables** permettant aux ing√©nieurs d√©butants d'exp√©rimenter concr√®tement les concepts cl√©s pr√©sent√©s dans le livre.

## üìã Liste des Scripts

| # | Script | Chapitre(s) | Concepts |
|---|--------|-----------|----------|
| 1 | `01_tokenization_embeddings.py` | 2 | Tokenisation, impact sur la longueur de s√©quence |
| 2 | `02_multihead_attention.py` | 3 | Self-attention, multi-head, poids d'attention |
| 3 | `03_temperature_softmax.py` | 7, 11 | Temp√©rature, softmax, entropie |
| 4 | `04_rag_minimal.py` | 13 | Pipeline RAG, retrieval, similarit√© cosinus |
| 5 | `05_pass_at_k_evaluation.py` | 12 | Pass@k, Pass^k, √©valuation de mod√®les |
| üéÅ **BONUS 1** | `06_react_agent_bonus.py` | 13, 14 | **Agents ReAct, framework g√©n√©rique, tool registration** |
| üéÅ **BONUS 2** | `07_llamaindex_rag_advanced.py` | 13 | **RAG avanc√©, document indexing, chat persistant** |
| üéÅ **BONUS 3** | `08_lora_finetuning_example.py` | 9 | **LoRA, QLoRA, comparaison fine-tuning, cas r√©el SNCF** |

## üöÄ D√©marrage Rapide

### 1. Cr√©er un environnement virtuel (recommand√©)

```bash
# Sur Windows
python -m venv venv
venv\Scripts\activate

# Sur macOS / Linux
python -m venv venv
source venv/bin/activate
```

### 2. Installer les d√©pendances

```bash
# Installation basique (pour les 5 scripts)
pip install torch transformers numpy scikit-learn

# Installation compl√®te (avec visualisations)
pip install torch transformers numpy scikit-learn matplotlib
```

### 3. Ex√©cuter un script

```bash
python 01_tokenization_embeddings.py
python 02_multihead_attention.py
python 03_temperature_softmax.py
python 04_rag_minimal.py
python 05_pass_at_k_evaluation.py
python 06_react_agent_bonus.py
python 07_llamaindex_rag_advanced.py
python 08_lora_finetuning_example.py
```

## üìñ D√©tails par Script

### Script 1 : Tokenisation et Embeddings (Chapitre 2)

**Voir:** `02-representation-texte-modeles-sequentiels.md`

Illustre :
- Comment les tokenizers (BPE, WordPiece) fragmentent le texte.
- L'impact du nombre de tokens sur le co√ªt computationnel.
- Les diff√©rences entre langues (fran√ßais vs anglais).

```bash
python 01_tokenization_embeddings.py
```

**Exemple de sortie:**
```
Texte: L'IA est utile
  Nombre de tokens: 6
  Token IDs: [1, 2, 3, 4, 5, 6]
  Tokens (texte): ['L', "'", 'IA', 'est', 'utile']

Texte court ‚Üí 2 tokens
Texte long (100x) ‚Üí 198 tokens
Facteur: 99.0x

‚ö†Ô∏è IMPLICATIONS:
  ‚Ä¢ Plus de tokens = plus de VRAM
  ‚Ä¢ Plus de tokens = latence plus √©lev√©e
  ‚Ä¢ Co√ªt d'inf√©rence ‚àù O(n¬≤) pour l'attention
```

---

### Script 2 : Multi-Head Attention (Chapitre 3)

**Voir:** `03-architecture-transformer.md`

Simule une couche d'attention multi-t√™te minimale :
- Projections Q, K, V.
- Calcul des scores d'attention.
- Visualisation de comment chaque t√™te focalise diff√©remment.

```bash
python 02_multihead_attention.py
```

**Exemple de sortie:**
```
Entr√©e x shape: (1, 4, 64)
  (batch=1, seq_len=4, d_model=64)

T√™te 0:
  Poids d'attention (apr√®s softmax):
    [[0.25 0.35 0.25 0.15]  # Le "regarde" 35% vers "chat"
     [0.10 0.60 0.20 0.10]  # "chat" regarde 60% vers "dort"
     ...

üí° INTUITION:
  ‚Ä¢ Chaque t√™te capture DIFF√âRENTES d√©pendances.
  ‚Ä¢ T√™te 0 peut se concentrer sur sujet-verbe.
  ‚Ä¢ T√™te 1 peut se concentrer sur verbe-adverbe.
```

---

### Script 3 : Temp√©rature et Softmax (Chapitres 7 & 11)

**Voir:** `07-preentrainement-llms.md`, `11-strategies-generation-inference.md`

Montre l'effet de la temp√©rature sur la distribution softmax :
- Basse T ‚Üí distribution pointue (greedy, d√©terministe).
- Haute T ‚Üí distribution plate (diversit√©, cr√©ativit√©).
- Lien avec l'entropie.

```bash
python 03_temperature_softmax.py
```

**Exemple de sortie:**
```
Temp√©rature = 0.1
  Probabilit√©s:
    chat:    0.874  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà...
    chien:   0.099  ‚ñà‚ñà
    souris:  0.019  
    oiseau:  0.008  
  Entropie: 0.347

Temp√©rature = 5.0
  Probabilit√©s:
    chat:    0.335  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    chien:   0.297  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    souris:  0.217  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
    oiseau:  0.151  ‚ñà‚ñà‚ñà‚ñà‚ñà
  Entropie: 1.358  (3.9x plus √©lev√©e!)

‚úì √Ä T=0.1 ‚Üí D√©terministe, repetitif.
‚úì √Ä T=5.0 ‚Üí Cr√©atif, mais risqu√©.
‚úì T=0.7-0.9 ‚Üí Bon compromis!
```

G√©n√®re optionnellement un graphique : `temperature_effect.png`

---

### Script 4 : RAG Minimaliste (Chapitre 13)

**Voir:** `13-systemes-augmentes-agents.md`

Simule un pipeline RAG complet :
1. **Retrieval** : chercher les 3 documents les plus pertinents.
2. **Augmentation** : injecter le contexte dans le prompt.
3. **G√©n√©ration** : le LLM r√©pond en s'appuyant sur le contexte.

```bash
python 04_rag_minimal.py
```

**Exemple de sortie:**
```
Question: "Comment fonctionne l'attention dans le Transformer?"

Top 3 documents r√©cup√©r√©s:
1. Score: 0.892
   L'attention multi-t√™te permet au mod√®le de regarder...

2. Score: 0.756
   Le Transformer est une architecture bas√©e sur l'attention...

Prompt augment√© envoy√© au LLM:
---
Vous √™tes un assistant expert.

Voici des documents pertinents:
- L'attention multi-t√™te permet...
- Le Transformer est une architecture...
- ...

Question: Comment fonctionne l'attention dans le Transformer?

R√©ponse bas√©e sur les documents:
---

COMPARAISON:
‚ùå SANS RAG:
  ‚Üí Hallucination possible
  ‚Üí Connaissances fig√©es
  ‚Üí Pas de sources √† v√©rifier

‚úÖ AVEC RAG:
  ‚Üí R√©ponses bas√©es sur des sources
  ‚Üí Acc√®s aux donn√©es externes
  ‚Üí Utilisateur peut v√©rifier les sources
```

---

### Script 5 : √âvaluation Pass@k (Chapitre 12)

**Voir:** `12-modeles-raisonnement.md`

√âvalue la fiabilit√© d'un mod√®le sur des t√¢ches v√©rifiables :
- **Pass@k** : probabilit√© d'au moins **une** r√©ussite en k tentatives.
- **Pass^k** : probabilit√© que **toutes** les k tentatives r√©ussissent.

```bash
python 05_pass_at_k_evaluation.py
```

**Exemple de sortie:**
```
Param√®tres:
  ‚Ä¢ Nombre de tentatives: 100
  ‚Ä¢ Probabilit√© de succ√®s: 30%

PASS@K (Au moins UNE r√©ussite en k tentatives):
Pass@1 = 30.0% (1 tentative)
Pass@3 = 65.7% (3 tentatives)
Pass@5 = 83.2% (5 tentatives)
Pass@10 = 97.2% (10 tentatives)

PASS^K (TOUTES les k tentatives r√©ussissent) ‚Äî STRICT:
Pass^1 = 30.0% (th√©orique: 0.3^1)
Pass^3 =  2.7% (th√©orique: 0.3^3)
Pass^5 =  0.2% (th√©orique: 0.3^5)

APPLICATION:
  ‚úì Recherche (HumanEval): Pass@k (diversit√©)
  ‚úì Agents critiques: Pass^k (fiabilit√© totale)
```

---

## üéØ Comment Utiliser Ces Scripts

### Pour les √âtudiants

1. **Lisez le chapitre pertinent du livre.**
2. **Ex√©cutez le script associ√©.**
3. **Modifiez les param√®tres** pour voir les effets :
   - Changez `seq_len`, `num_heads`, `temperatures`, etc.
   - Ajoutez vos propres textes/documents.
4. **Ajoutez des `print()`** pour d√©boguer et comprendre les dimensions.

### Pour les Ing√©nieurs

- Utilisez ces scripts comme **point de d√©part** pour vos impl√©mentations.
- Int√©grez-les dans des **pipelines de production** (RAG, √©valuation, etc.).
- Adaptez le code √† votre **infrastructure** (GPUs, APIs, bases de donn√©es).

## üéÅ Bonus Scripts

### BONUS 1 : ReAct Agent Framework (`06_react_agent_bonus.py`)

**Voir:** `REACT_AGENT_INTEGRATION.md`

Framework complet pour construire des **agents autonomes avec pattern ReAct**.

**Caract√©ristiques:**
- ‚úÖ Classe `Agent` r√©utilisable et extensible
- ‚úÖ Syst√®me de registration d'outils (tool definition)
- ‚úÖ Boucle Thought ‚Üí Action ‚Üí Observation
- ‚úÖ LLM simulation (pr√™t pour OpenAI, Claude, Groq, Ollama)
- ‚úÖ Historique et gestion d'it√©rations
- ‚úÖ Exemple avec 3 outils (calculator, date, knowledge base)

**Concepts couverts:**
- Agents autonomes (Chapitre 13)
- Protocoles standards agentiques (Chapitre 14)
- Pattern ReAct (Reasoning + Acting)
- Tool calling et execution

**Ex√©cution:**
```bash
python 06_react_agent_bonus.py
```

**Int√©gration avec LLMs r√©els:**
```python
from openai import OpenAI

class OpenAIAgent(Agent):
    def _simulate_llm_reasoning(self, task, context):
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content
```

Voir `REACT_AGENT_INTEGRATION.md` pour int√©grations compl√®tes (OpenAI, Claude, Groq, Ollama).

---

### BONUS 2 : LlamaIndex RAG Avanc√© (`07_llamaindex_rag_advanced.py`)

**Voir:** `LLAMAINDEX_GUIDE.md`

Framework complet pour construire des **syst√®mes RAG avanc√©s avec LlamaIndex**.

**Caract√©ristiques:**
- ‚úÖ Indexation vectorielle d'documents
- ‚úÖ Retrieval avanc√© (similarity search, hybrid BM25+vector)
- ‚úÖ RAG Engine avec augmentation de contexte
- ‚úÖ Chatbot avec m√©moire conversationnelle
- ‚úÖ √âvaluation de qualit√© (Precision, Recall, F1)
- ‚úÖ Export des r√©sultats en JSON
- ‚úÖ Fallback embeddings simul√©s (pas de d√©pendances requises)

**Concepts couverts:**
- RAG (Retrieval-Augmented Generation) - Chapitre 13
- Document parsing et indexing
- Vector similarity search
- Query augmentation avec contexte
- Conversation avec persistance

**Phases d'ex√©cution:**
1. Chargement des documents
2. Cr√©ation de l'index vectoriel
3. Initialisation du RAG Engine
4. Requ√™tes RAG avec retrieval
5. Chat avec m√©moire
6. √âvaluation de qualit√©
7. Export des r√©sultats

**Ex√©cution sans d√©pendances:**
```bash
python 07_llamaindex_rag_advanced.py
```
‚ö†Ô∏è Utilise embeddings simul√©s (d√©terministes).

**Ex√©cution avec LlamaIndex r√©el:**
```bash
pip install llama-index openai
python 07_llamaindex_rag_advanced.py
```
‚úì Utilise OpenAI embeddings (text-embedding-3-small).

**Production avec documents r√©els:**
```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# Charger PDFs, Word, HTML, etc.
documents = SimpleDirectoryReader("./docs").load_data()

# Cr√©er index
index = VectorStoreIndex.from_documents(documents)

# Query engine
query_engine = index.as_query_engine(similarity_top_k=3)
response = query_engine.query("Votre question ici")
```

Voir `LLAMAINDEX_GUIDE.md` pour :
- Installation compl√®te (LlamaIndex, vector stores, readers)
- Int√©gration OpenAI, Claude, Groq
- Hybrid search (BM25 + vectoriel)
- Agents avec outils
- √âvaluation de qualit√©
- Cas d'usage avanc√©s

---

### Script 8 : LoRA & QLoRA Fine-tuning (Chapitre 9) üéÅ BONUS 3

**Voir:** `09-affinage-supervise-sft.md`

D√©montre les techniques de fine-tuning efficace en ressources :
- LoRA (Low-Rank Adaptation) : r√©duction des param√®tres entra√Ænables.
- QLoRA (Quantized LoRA) : quantification + LoRA pour VRAM ultra-faible.
- Comparaison chiffr√©e : Full Fine-tuning vs LoRA vs QLoRA.
- Cas r√©el : adaptation d'un mod√®le LLaMA-7B pour le domaine ferroviaire (SNCF).

```bash
python 08_lora_finetuning_example.py
```

**Exemple de sortie:**
```
=== LoRA Calculations ===
LLaMA-7B (7B params total)
  LoRA Rank 64:
    Trainable params (A+B): 85,262,336 (1.22% of model)
    Reduction: 81.7√ó

=== Fine-tuning Method Comparison ===
Method          | VRAM Needed | Time (10K ex) | Checkpoint | Use Case
Full FT         | 28 GB       | 8h            | 26 GB      | Unlimited budget
LoRA            | 8 GB        | 2.5h          | 85 MB      | Multi-domain, quick
QLoRA           | 2 GB        | 3h            | 85 MB      | Single GPU edge

=== Real Case: SNCF Railway Adapter ===
Scenario: Adapt LLaMA-7B for railway maintenance (10K domain Q&A)
Hardware: RTX 4090 (24GB VRAM)

Full Fine-tuning:  Need 28GB ‚Üí IMPOSSIBLE on RTX 4090
LoRA:              Need 8GB  ‚Üí ‚úÖ Feasible, 2.5h training
QLoRA:             Need 2GB  ‚Üí ‚úÖ Feasible, 3h training, leaves GPU RAM free
```

Concepts abord√©s :
- W = W‚ÇÄ + BA (d√©composition LoRA)
- Effet du rank (8, 16, 32, 64) sur taille vs performance
- Quantisation 8-bit et √©conomies de m√©moire
- Peft library integration (transformers + peft)
- Pseudo-code pour adapter mod√®les multilingues

---

## üìö Correspondance Livre ‚Üî Scripts

| Chapitre | Topic | Script |
|----------|-------|--------|
| 2 | Tokenisation, Embeddings | `01_tokenization_embeddings.py` |
| 3 | Architecture Transformer, Attention | `02_multihead_attention.py` |
| 7 | Pr√©-entra√Ænement, Loss | `03_temperature_softmax.py` |
| 9 | Affinage supervis√©, LoRA, QLoRA | **`08_lora_finetuning_example.py`** |
| 11 | Strat√©gies de g√©n√©ration, Temp√©rature | `03_temperature_softmax.py` |
| 12 | Mod√®les de raisonnement, √âvaluation | `05_pass_at_k_evaluation.py` |
| 13 | Syst√®mes augment√©s, RAG, Agents | `04_rag_minimal.py`, **`06_react_agent_bonus.py`**, **`07_llamaindex_rag_advanced.py`** |
| 14 | Protocoles standards agentiques | **`06_react_agent_bonus.py`** |

## üõ†Ô∏è Troubleshooting

### "ModuleNotFoundError: No module named 'transformers'"

```bash
pip install transformers
```

### "ModuleNotFoundError: No module named 'torch'"

```bash
# CPU
pip install torch

# GPU (CUDA 12.1)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### Matplotlib non install√©

```bash
pip install matplotlib
# Script 3 continuera √† fonctionner sans, mais pas de graphique.
```

### Script trop lent (transformers qui t√©l√©charge un mod√®le)

- Les mod√®les se t√©l√©chargent automatiquement √† la premi√®re ex√©cution (~3 GB pour LLaMA).
- Les prochaines ex√©cutions seront plus rapides (cache local).
- Alternative : utiliser un mod√®le plus petit (`distilbert-base-multilingual-cased`).

## üìù Notes

- **Pas de GPU requis** : tous les scripts tournent sur CPU (plus lentement).
- **D√©pendances minimales** : seulement `numpy`, `torch`, `transformers`, `scikit-learn`.
- **Code √©ducatif** : les scripts privil√©gient la clart√© sur l'optimisation.
- **Compatible Python 3.9+**.

## ü§ù Contribution

Si tu souhaites ajouter un script ou corriger un bug, n'h√©site pas √† :
1. Fork ce repository.
2. Cr√©e une branche (`git checkout -b feature/mon-script`).
3. Commit et pousse (`git push origin feature/mon-script`).
4. Ouvre une pull request.

---

**Bon apprentissage! üöÄ**
