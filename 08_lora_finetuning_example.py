#!/usr/bin/env python
"""
Script 8 : LoRA et QLoRA - Fine-tuning Efficace (Chapitre 9).

Ce script d√©montre comment utiliser LoRA (Low-Rank Adaptation) et QLoRA
pour fine-tuner efficacement un mod√®le de langage sans avoir besoin de 
ressources GPU immenses.

Concepts couverts :
- LoRA : adaptateurs de petit rang au lieu de fine-tuning complet
- QLoRA : LoRA sur mod√®les quantifi√©s 4-bit (r√©volution d'accessibilit√©)
- Comparaison des ressources (VRAM, temps, param√®tres)
- Cas d'usage r√©el : adaptation √† un domaine sp√©cifique

D√©pendances minimales (sans r√©el GPU requis pour la d√©mo):
    pip install torch numpy

D√©pendances pour fine-tuning r√©el:
    pip install torch transformers peft bitsandbytes

Utilisation :
    python 08_lora_finetuning_example.py
"""

import sys
from typing import Dict, List, Tuple


def calculate_lora_parameters(
    model_dim: int,
    lora_rank: int,
    num_layers: int
) -> Dict[str, int]:
    """
    Calculer le nombre de param√®tres suppl√©mentaires pour LoRA.
    
    LoRA ajoute deux matrices par couche (Q et V, g√©n√©ralement) :
    - Matrice A : model_dim √ó lora_rank
    - Matrice B : lora_rank √ó model_dim
    
    Total par couche : 2 √ó model_dim √ó lora_rank
    """
    params_per_layer = 2 * model_dim * lora_rank
    total_lora_params = params_per_layer * num_layers
    
    return {
        "params_per_layer": params_per_layer,
        "total_lora_params": total_lora_params,
        "percentage_of_model": None  # Sera calcul√© plus bas
    }


def compare_finetuning_methods(
    model_size: int,
    model_dim: int,
    num_layers: int,
    lora_rank: int = 8
) -> Dict[str, Dict]:
    """
    Comparer trois m√©thodes de fine-tuning en termes de :
    - Param√®tres entra√Ænables
    - M√©moire VRAM approximative
    - Temps d'entra√Ænement relatif
    """
    
    # === Full Fine-tuning ===
    full_params = model_size
    # R√®gle empirique: VRAM ‚âà 4 √ó param√®tres (pour optimiseur Adam + gradients)
    full_vram_gb = (full_params * 4) / (1024**3)
    full_time_relative = 1.0  # R√©f√©rence
    
    # === LoRA ===
    lora_calc = calculate_lora_parameters(model_dim, lora_rank, num_layers)
    lora_params = lora_calc["total_lora_params"]
    lora_percentage = (lora_params / full_params) * 100
    # LoRA : sauvegarder le mod√®le original + gradients sur LoRA seulement
    lora_vram_gb = (full_params + lora_params * 4) / (1024**3)
    lora_time_relative = 0.3  # Empirique : plus rapide car moins de params √† mettre √† jour
    
    # === QLoRA ===
    # QLoRA quantifie le mod√®le en 4-bit, donc 4x moins de m√©moire pour le mod√®le
    # + sauvegarder LoRA weights
    qlora_vram_gb = (full_params / 4 + lora_params * 4) / (1024**3)
    qlora_time_relative = 0.4  # L√©g√®rement plus lent que LoRA (overhead quantification)
    
    return {
        "full_fine_tuning": {
            "trainable_params": full_params,
            "param_percentage": 100.0,
            "vram_gb": full_vram_gb,
            "time_relative": full_time_relative,
            "pros": "Meilleure performance",
            "cons": "Tr√®s gourmand en VRAM et temps"
        },
        "lora": {
            "trainable_params": lora_params,
            "param_percentage": lora_percentage,
            "vram_gb": lora_vram_gb,
            "time_relative": lora_time_relative,
            "pros": "Bon compromis performance/ressources",
            "cons": "N√©cessite quand m√™me pas mal de VRAM"
        },
        "qlora": {
            "trainable_params": lora_params,
            "param_percentage": lora_percentage,
            "vram_gb": qlora_vram_gb,
            "time_relative": qlora_time_relative,
            "pros": "R√âVOLUTION : fine-tune 65B sur 1 GPU",
            "cons": "L√©g√®rement plus lent (quantification)"
        }
    }


def main():
    print("=" * 80)
    print("LORA & QLORA : FINE-TUNING EFFICACE")
    print("=" * 80)
    print()
    
    # === Exemple 1 : LLaMA 7B ===
    print("=" * 80)
    print("EXEMPLE 1 : Fine-tuner LLaMA-7B")
    print("=" * 80)
    print()
    
    llama_7b_params = 7_000_000_000  # 7 milliards de param√®tres
    llama_dim = 4096              # Dimension des embeddings
    llama_layers = 32             # Nombre de couches
    lora_rank = 8                 # Rang LoRA standard
    
    results_7b = compare_finetuning_methods(
        llama_7b_params, llama_dim, llama_layers, lora_rank
    )
    
    print(f"Mod√®le : LLaMA-7B ({llama_7b_params / 1e9:.1f}B param√®tres)")
    print(f"LoRA rank : {lora_rank}")
    print()
    
    print("Comparaison des m√©thodes :")
    print("-" * 80)
    print(f"{'M√©thode':<20} {'Params':<20} {'VRAM':<12} {'Temps':<10} {'Cas d\'usage'}")
    print("-" * 80)
    
    for method, data in results_7b.items():
        params_M = data["trainable_params"] / 1e6
        vram = data["vram_gb"]
        time_rel = data["time_relative"]
        print(f"{method:<20} {params_M:>15.1f}M {vram:>10.1f}GB {time_rel:>8.1f}x {'‚Üí ' + data['pros']}")
    
    print()
    print("INSIGHT :")
    print("  ‚Ä¢ Full fine-tuning : 28 GB VRAM ‚Üí n√©cessite A100 ou RTX 6000")
    print("  ‚Ä¢ LoRA : 8 GB VRAM ‚Üí entra√Ænable sur RTX 4090 (24 GB)")
    print("  ‚Ä¢ QLoRA : 2 GB VRAM ‚Üí entra√Ænable sur RTX 3090 (24 GB) ‚úÖ R√âVOLUTION!")
    print()
    
    # === Exemple 2 : LLaMA 65B (le cas d'usage r√©el de QLoRA) ===
    print()
    print("=" * 80)
    print("EXEMPLE 2 : Fine-tuner LLaMA-65B (le vrai cas d'usage de QLoRA)")
    print("=" * 80)
    print()
    
    llama_65b_params = 65_000_000_000  # 65 milliards
    llama_65b_dim = 8192
    llama_65b_layers = 80
    
    results_65b = compare_finetuning_methods(
        llama_65b_params, llama_65b_dim, llama_65b_layers, lora_rank
    )
    
    print(f"Mod√®le : LLaMA-65B ({llama_65b_params / 1e9:.0f}B param√®tres)")
    print(f"LoRA rank : {lora_rank}")
    print()
    
    print("Comparaison des m√©thodes :")
    print("-" * 80)
    print(f"{'M√©thode':<20} {'Params':<20} {'VRAM':<12} {'Temps':<10}")
    print("-" * 80)
    
    for method, data in results_65b.items():
        params_M = data["trainable_params"] / 1e6
        vram = data["vram_gb"]
        time_rel = data["time_relative"]
        accessible = "‚ùå 260GB" if method == "full_fine_tuning" else "‚ö†Ô∏è  32GB" if method == "lora" else "‚úÖ 8GB"
        print(f"{method:<20} {params_M:>15.1f}M {vram:>10.1f}GB {time_rel:>8.1f}x  {accessible}")
    
    print()
    print("R√âV√âLATION :")
    print("  ‚Ä¢ Full fine-tuning : 260 GB VRAM ‚Üí IMPOSSIBLE (m√™me pas un cluster GPU)")
    print("  ‚Ä¢ LoRA : 32 GB VRAM ‚Üí A100 ou deux RTX 4090 (possible mais co√ªteux)")
    print("  ‚Ä¢ QLoRA : 8 GB VRAM ‚Üí RTX 3090 SIMPLE (2024‚Ç¨ d'occasion) ‚úÖ‚úÖ‚úÖ")
    print()
    print("  ‚Üí QLoRA a d√©mocratis√© l'acc√®s aux mod√®les LLM g√©ants!")
    print()
    
    # === Cas d'usage pratique ===
    print()
    print("=" * 80)
    print("CAS D'USAGE R√âEL : Adapter LLaMA-7B pour ton domaine m√©tier")
    print("=" * 80)
    print()
    
    print("Sc√©nario : Vous travaillez chez SNCF et voulez adapter LLaMA-7B")
    print("          pour r√©pondre √† des questions sur la maintenance ferroviaire.")
    print()
    
    print("Approche LoRA :")
    print("-" * 80)
    print("""
    1. Charger LLaMA-7B (13 GB en full precision)
    2. Ajouter adaptateurs LoRA (85 MB seulement !)
    3. Fine-tuner sur votre dataset SNCF (ex: 10K paires Q/A)
    4. Pendant l'entra√Ænement :
       - Sauvegarder seulement les 85 MB de LoRA (pas 13 GB)
       - VRAM n√©cessaire : ~8 GB (sur RTX 4090)
       - Temps : ~2h au lieu de ~8h en full fine-tuning
    5. En inf√©rence :
       - Charger LLaMA-7B + 85 MB de LoRA
       - Performance : quasi-identique au full fine-tuning
       - Latence : IDENTIQUE (fusion optionnelle pour vitesse)
    
    R√©sultat : Un mod√®le expert SNCF sans d√©penser 100k‚Ç¨ en GPU!
    """)
    
    print()
    print("=" * 80)
    print("CODE PRATIQUE (pseudocode)")
    print("=" * 80)
    print()
    
    code_example = '''
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import get_peft_model, LoraConfig, TaskType

# Charger le mod√®le de base
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    device_map="auto",  # Distribue le mod√®le sur les GPUs disponibles
)

# Configurer LoRA
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                          # Rang LoRA
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"],  # Couches √† adapter
)

# Appliquer LoRA au mod√®le
model = get_peft_model(model, lora_config)

# Afficher la r√©duction
model.print_trainable_parameters()
# Output: trainable params: 4,194,304 || all params: 6,738,415,616
#         Trainable%: 0.06%

# Fine-tuner seulement avec votre dataset
trainer = Trainer(
    model=model,
    train_dataset=train_dataset,
    args=TrainingArguments(output_dir="./lora_checkpoint"),
)
trainer.train()

# Sauvegarder SEULEMENT LoRA (85 MB)
model.save_pretrained("./sncf_lora_weights")

# En inf√©rence, charger et fusionner
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")
model = PeftModel.from_pretrained(model, "./sncf_lora_weights")
model = model.merge_and_unload()  # Fusionner (optionnel, pour vitesse)
'''
    
    print(code_example)
    
    print()
    print("=" * 80)
    print("R√âSUM√â")
    print("=" * 80)
    print()
    print("‚úÖ LoRA/QLoRA = r√©volution d'accessibilit√©")
    print("   - Fine-tune des mod√®les g√©ants sans cluster GPU")
    print("   - Sauvegarder seulement quelques MB au lieu de GB")
    print("   - Performance quasi-identique au full fine-tuning")
    print()
    print("‚ö†Ô∏è  Quand utiliser quoi :")
    print("   - LoRA : petit mod√®le (7-13B) + GPU mid-range (RTX 4090)")
    print("   - QLoRA : mod√®le g√©ant (65B+) + GPU basic (RTX 3090)")
    print("   - Full fine-tuning : donn√©es TR√àS grandes (>1M exemples) + infra GPU massive")
    print()
    print("üí° Conseil : Commencez TOUJOURS par LoRA. C'est le sweet spot.")
    print()


if __name__ == "__main__":
    main()
